{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Minitarea5[1].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-19T18:30:18.109327Z",
          "start_time": "2020-03-19T18:30:18.103344Z"
        },
        "colab_type": "text",
        "id": "q5CSRY4oNCHK"
      },
      "source": [
        "\n",
        "# Minitarea 5\n",
        "\n",
        "\n",
        "-----------------\n",
        "\n",
        "Nombre: Ignacio Díaz Lara\n",
        "\n",
        "Fecha de Entrega: 30 de Julio, 2020\n",
        "\n",
        "\n",
        "## Instrucciones\n",
        "\n",
        "- El ejercicio consiste en:\n",
        "\n",
        "    - Responder preguntas relativas a los contenidos vistos en los vídeos y slides de las clases. \n",
        "    \n",
        "    - Utilizar la librería Transformers\n",
        "\n",
        "- La minitarea es INDIVIDUAL.\n",
        "\n",
        "- Está demás decir que no se admiten copias, ni de código, ni de respuestas escritas. \n",
        "\n",
        "- La entrega debe ser por u-cursos.\n",
        "\n",
        "- Atrasos: se descontará un punto por día hábil de atraso tanto para las mini-tareas como para las competencias.\n",
        "\n",
        "- En el horario de auxiliar se abrirán horarios de consulta en donde podrán preguntar acerca del ejercicio y en general, de todo el curso. \n",
        "\n",
        "- Cada sección tiene un punto base y se evalúa sobre 6 puntos.\n",
        "\n",
        "- Al revisar, tu código será ejecutado. Verifica que tu entrega no tenga errores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G4wYf0vgnbTv"
      },
      "source": [
        "## Preguntas Teóricas\n",
        "Para estas preguntas no es necesario implementar código, pero pueden utilizar pseudo código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B5hUG6-8ngoK"
      },
      "source": [
        "### Arquitecturas de Redes Neuronales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "irsqBVmCnx3M"
      },
      "source": [
        "**Pregunta 1**: Explique el principal problema de las redes Elman recurrentes. Explique cada compuerta de las redes LSTM y la GRU.  (1,5 puntos)\n",
        "\n",
        "**Respuesta**: El gran problema de las redes Elman es el llamado problema del gradiente desvaneciente (vanishing gradients problem). Cuando se tiene redes muy profundas, ocurre que los gradientes tienden a achicarse a medida que se va a las primeras capas por backpropagation, haciendo cada vez más dificultoso que influyan en las palabras del principio de la red. La razón de este desvanecimiento tiene que ver con que los gradientes tienen repetidas multiplicaciones de la misma matriz W ya que se usa la misma W para todas las capas. De esta manera, si la dependencia es muy larga, le va a costar más reconocer pratones a las redes Elman.\n",
        "\n",
        "Las compuertas de LSTM son todas sigmoides que tienen parámetros lineales del input y del estado. Son: i (Input), que toma el input y selecciona qué cosas se van a conservar en el nuevo estado (cell state); f (Forget), que toma el input y selecciona que cosas va a olvidar el nuevo estado (cell state); o (Output), que dice que cosas del input tiene que pasar al output (en la working memory).\n",
        "\n",
        "Las compuertas de GRU son sigmoides que tienen parámetros lineales del input y el estado anterior. Son: z (Update gate), que controla las proporciones de la interpolación del estado nuevo y el antiguo; y r (Reset gate), que se usa para tener control del estado previo (indica que preservar de éste) y computar un estado nuevo (que sirve de output también).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G5FaWqBVvL90"
      },
      "source": [
        "**Pregunta 2**: Explique cuales son las diferencias entre las tres arquitecturas de sequence to sequence vistas en clases (Encoder-Decoder con RNN, Encoder-Decoder con RNN y Attention, y el Transformer) (1,5 puntos)\n",
        "\n",
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9q0SrZS8CqX",
        "colab_type": "text"
      },
      "source": [
        "**Pregunta 3**: ¿Cúal es el principal diferencia entre los Embeddings contextualizados (por ejemplo BERT) vs. los Embeddings estáticos (por ejemplo Word2Vec)? (1,5 puntos)\n",
        "\n",
        "**Respuesta**: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX0C2x5j8DNR",
        "colab_type": "text"
      },
      "source": [
        "**Pregunta 4**: Explique en que tareas y las arquitecturas con las que se entrenan ELMO y BERT (1,5 puntos)\n",
        "\n",
        "**Respuesta**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ocS_vQhR1gcU"
      },
      "source": [
        "## Preguntas Prácticas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ol82nJ0FnmcP"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W28UEwvGYTbg",
        "colab_type": "text"
      },
      "source": [
        "Lo primero es instalar las librerías necesarias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X4Gbx7wYWDD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "d3bc74f6-f6f6-478f-bdda-ecb5426076ee"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertForNextSentencePrediction, BertForMaskedLM, BertForQuestionAnswering\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIQo-VciYz2V",
        "colab_type": "text"
      },
      "source": [
        "Para las preguntas que siguen, utilizaremos distintas variantes de BERT disponibles en la librería transformers. [Aquí](https://huggingface.co/transformers/model_doc/bert.html) pueden encontrar toda la documentación necesaria. El modelo pre-entrenado a utilizar es \"bert-base-uncased\" (salvo para question answering)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4_WHbz8bXx2",
        "colab_type": "text"
      },
      "source": [
        "BERT es un modelo de lenguaje que fue entrenado exhaustivamente sobre dos tareas: 1) Next sentence prediction. 2) Masked language modeling. Veremos ejemplos de esto en la Auxiliar del jueves 16/07/20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyMb4YZRMYkm",
        "colab_type": "text"
      },
      "source": [
        "#### BertForNextSentencePrediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt6CbBtyadRb",
        "colab_type": "text"
      },
      "source": [
        "**Pregunta 1** (2 puntos en total): Utilizando el modelo BertForNextSentencePrediction de la librería transformers, muestre cual de las 2 oraciones es más probable que sea una continuación de la primera. (0.5 puntos cada una)\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "Initial: \"The sky is blue.\"\\\n",
        "A: \"This is due to the shorter wavelength of blue light.\"\\\n",
        "B: \"Chile is one of the world's greatest economies.\"\n",
        "\n",
        "Debería retornar \"La oración que continúa más probable es A\", justificándolo con la evaluación de BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOX0bwser8OM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "366bd42c-e5de-462e-f352-6e54eb2c4ae2"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoBKxPt-mz-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funciones auxiliares:\n",
        "def oracion_mas_probable(first,sentA,sentB):\n",
        "  encoding = tokenizer(first, sentA, return_tensors='pt')\n",
        "  encoding2 = tokenizer(first, sentB, return_tensors='pt')\n",
        "  loss, logits = model(**encoding, next_sentence_label=torch.LongTensor([1]))\n",
        "  loss2, logits2 = model(**encoding2, next_sentence_label=torch.LongTensor([1]))\n",
        "  if logits[0, 0] < logits2[0, 0]:\n",
        "    print(sentB)\n",
        "  elif logits[0,0] > logits2[0, 0]:\n",
        "    print(sentA)\n",
        "  else:\n",
        "    print(\"Son equiprobables\")\n",
        "  \n",
        "#referencia: auxiliar 5"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goXIGaief8Bi",
        "colab_type": "text"
      },
      "source": [
        "1.1)\n",
        "Initial: \"My cat is fluffy.\"\\\n",
        "A: \"My dog has a curling tail.\"\\\n",
        "B: \"A song can make or ruin a person’s day if they let it get to them.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zea5pybzf9x1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3dc8537b-5189-4f57-9179-727b4dc032ef"
      },
      "source": [
        "initial11 = \"My cat is fluffy.\"\n",
        "a11 = \"My dog has a curling tail.\"\n",
        "b11 = \"A song can make or ruin a person’s day if they let it get to them.\"\n",
        "oracion_mas_probable(initial11, a11, b11)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My dog has a curling tail.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHd7UzXpgCY-",
        "colab_type": "text"
      },
      "source": [
        "1.2)\n",
        "Initial: \"The Big Apple is famous worldwide.\"\\\n",
        "A: \"You can add cinnamon for the perfect combination.\"\\\n",
        "B: \"It is America's largest city.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7XbCBDmgCJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ee08bb3c-edd7-4db9-f389-1cd49dbe3e98"
      },
      "source": [
        "initial12 = \"The Big Apple is famous worldwide.\"\n",
        "a12 = \"You can add cinnamon for the perfect combination.\"\n",
        "b12 = \"It is America's largest city.\"\n",
        "oracion_mas_probable(initial12, a12, b12)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It is America's largest city.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1Y7oxM7d0jM",
        "colab_type": "text"
      },
      "source": [
        "1.3)\n",
        "Initial: \"Roses are red.\"\\\n",
        "A: \"Violets are blue.\"\\\n",
        "B: \"Fertilize them regularly for impressive flowers.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6t1QxlOf-O7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "060caacf-133d-4a6d-b11f-2495f5ee05cd"
      },
      "source": [
        "initial13 = \"Roses are red.\"\n",
        "a13 = \"Violets are blue.\"\n",
        "b13 = \"Fertilize them regularly for impressive flowers.\"\n",
        "oracion_mas_probable(initial13, a13, b13)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Violets are blue.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgC5BcoDiIWV",
        "colab_type": "text"
      },
      "source": [
        "1.4)\n",
        "Initial: \"I play videogames the whole day.\"\\\n",
        "A: \"They make me happy.\"\\\n",
        "B: \"They make me rage.\"\\\n",
        "<sub><sup><sub><sup>No estoy tan de acuerdo :D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJK7PhmwiItA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0a8a4eed-05e1-43ed-fbcd-c0c48cfd0b4c"
      },
      "source": [
        "initial14 = \"I play videogames the whole day.\"\n",
        "a14 = \"They make me happy.\"\n",
        "b14 = \"They make me rage.\"\n",
        "oracion_mas_probable(initial14, a14, b14)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "They make me happy.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ArRo_zMdtr",
        "colab_type": "text"
      },
      "source": [
        "####BertForMaskedLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCN8XM7qihx1",
        "colab_type": "text"
      },
      "source": [
        "**Pregunta 2** (2 puntos en total): Ahora utilizaremos BertForMaskedLM para predecir una palabra oculta en una oración. (0.5 puntos cada una)\\\n",
        "Por ejemplo:\\\n",
        "BERT input: \"I want to _ a new car.\"\\\n",
        "BERT prediction: \"buy\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZwFxbJOv_aW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "c86f8795-946e-461f-ecc2-3127f4fc991f"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSGkdSokv-3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funcion auxiliar\n",
        "def palabra_mas_probable(sentence):\n",
        "  tokenized_text = tokenizer.tokenize(sentence)\n",
        "  masked_index = tokenized_text.index(\"[MASK]\")\n",
        "  #separa frase pero como sólo tenemos una sentencia basta que sea una\n",
        "  #lista homogenea\n",
        "  segments_ids = [0]*len(tokenized_text)\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        " \n",
        "  # Predict all tokens\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "  predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
        "  predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "  return predicted_token\n",
        "\n",
        "# referencia: tutorial quickstart de huggingface para Bert"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gua8HPhfkYOb",
        "colab_type": "text"
      },
      "source": [
        "2.1)\n",
        "BERT input: \"[CLS] I love [MASK] . [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRFJnURk7ut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0b0d1f7c-765f-4713-9cdf-e3aa5d51b518"
      },
      "source": [
        "palabra_mas_probable(\"[CLS] I love [MASK] . [SEP]\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7npsRd9Nk8hi",
        "colab_type": "text"
      },
      "source": [
        "2.2)\n",
        "BERT input: \"[CLS] I hear that Karen is very [MASK] . [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW7CosmKk87e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "848241c7-f050-4c11-e463-7c74a9cd0a0f"
      },
      "source": [
        "palabra_mas_probable(\"[CLS] I hear that Karen is very [MASK] . [SEP]\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'upset'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SqzWh8Wk9TD",
        "colab_type": "text"
      },
      "source": [
        "2.3)\n",
        "BERT input: \"[CLS] She had the gift of being able to [MASK] . [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6YFd1Xpk9qd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a4ce22d3-468d-45f1-a0fc-3912e670feab"
      },
      "source": [
        "palabra_mas_probable(\"[CLS] She had the gift of being able to [MASK] . [SEP]\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'fly'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoV_5suNk-1X",
        "colab_type": "text"
      },
      "source": [
        "2.4)\n",
        "BERT input: \"[CLS] It's not often you find a [MASK] on the street. [SEP]\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8ayI5VIk_Hr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ad024d99-e40f-40d9-fea2-7a3022c4cf34"
      },
      "source": [
        "palabra_mas_probable(\"[CLS] It's not often you find a [MASK] on the street. [SEP]\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'car'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJFPpgoYMim5",
        "colab_type": "text"
      },
      "source": [
        "####BertForQuestionAnswering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmCSmftALYbA",
        "colab_type": "text"
      },
      "source": [
        "**Pregunta 3**: (2 puntos) Utilizando el modelo BertForQuestionAnswering pre-entrenado con 'bert-large-uncased-whole-word-masking-finetuned-squad', extraiga la respuesta a cada una de las siguientes 4 preguntas y su contexto. (0.5 puntos cada una). Recuerde cambiar el tokenizer para que coincida con el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UhKkKpyToFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV8dZINJTrPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funcion Auxiliar\n",
        "def entregar_respuesta(qst, cntxt):\n",
        "  #Su código aquí\n",
        "  input_text = \"[CLS] \" + qst + \" [SEP] \" + cntxt + \" [SEP]\"\n",
        "  input_ids = tokenizer.encode(input_text)\n",
        "  token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))] \n",
        "  start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
        "  all_tokens = tokenizer.convert_ids_to_tokens(input_ids)  \n",
        "  result = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
        "  print(result.replace(\" ##\",\"\"))\n",
        "# referencia: tutorial de huggingface para BertForQuestionAnswering"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7zizotdKVIF",
        "colab_type": "text"
      },
      "source": [
        "3.1)\n",
        "\n",
        "Pregunta: \"When was the Battle of Iquique?\"\n",
        "\n",
        "Contexto: \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_TrDijrKnQH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2d5b0329-2822-4d90-af32-ed7adb29771a"
      },
      "source": [
        "question = \"When was the Battle of Iquique?\"\n",
        "context = \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\"\n",
        "entregar_respuesta(question, context)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21 may 1879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0au9XCqNB2TY",
        "colab_type": "text"
      },
      "source": [
        "3.2)\n",
        "\n",
        "Pregunta: \"Who won the Battle of Iquique?\"\n",
        "\n",
        "Contexto: \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DlTUMxAB_0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ef521904-3e98-4ec1-92fc-397fc942d3c4"
      },
      "source": [
        "question = \"Who won the Battle of Iquique?\"\n",
        "context = \"The Battle of Iquique was a naval engagement that occurred between a Chilean corvette under the command of Arturo Prat and a Peruvian ironclad under the command of Miguel Grau Seminario on 21 May 1879, during the naval stage of the War of the Pacific, and resulted in a Peruvian victory.\"\n",
        "entregar_respuesta(question, context)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "peruvian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryRajxniCKbp",
        "colab_type": "text"
      },
      "source": [
        "3.3)\n",
        "\n",
        "Pregunta: \"Who introduced peephole connections to LSTM networks?\"\n",
        "\n",
        "Contexto: \"What I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them. One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1rT9kgLCKuy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "752ec1c2-3209-4f9f-9cec-4a18fd7572f7"
      },
      "source": [
        "question = \"Who introduced peephole connections to LSTM networks?\"\n",
        "context = \"What I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them. One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state.\"\n",
        "entregar_respuesta(question, context)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gers & schmidhuber\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqn0vNvKCLAq",
        "colab_type": "text"
      },
      "source": [
        "3.4)\n",
        "\n",
        "Pregunta: \"When is the cat most active?\"\n",
        "\n",
        "Contexto: \"The cat is similar in anatomy to the other felid species: it has a strong flexible body, quick reflexes, sharp teeth and retractable claws adapted to killing small prey. Its night vision and sense of smell are well developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling and grunting as well as cat-specific body language. It is a solitary hunter but a social species. It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals. It is a predator that is most active at dawn and dusk. It secretes and perceives pheromones.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5n37FwOCLTG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fa2d2194-db98-49d2-95f3-dfeb445f6da5"
      },
      "source": [
        "question = \"When is the cat most active?\"\n",
        "context = \"The cat is similar in anatomy to the other felid species: it has a strong flexible body, quick reflexes, sharp teeth and retractable claws adapted to killing small prey. Its night vision and sense of smell are well developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling and grunting as well as cat-specific body language. It is a solitary hunter but a social species. It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals. It is a predator that is most active at dawn and dusk. It secretes and perceives pheromones.\"\n",
        "entregar_respuesta(question, context)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dawn and dusk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxPeiS73Qyxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}