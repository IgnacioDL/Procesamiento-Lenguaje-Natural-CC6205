{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"_18767932_assignment_1_f.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-31T13:49:08.174519Z","start_time":"2020-03-31T13:49:08.165989Z"},"id":"DKFrJAgZXqzZ","colab_type":"text"},"source":["# Tarea 1 NLP : Competencia de Clasificaci√≥n de Texto\n","-------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"WGICQIOfXqzd","colab_type":"text"},"source":["- **Nombre:** Ignacio D√≠az Lara, Felipe Manen N√∫√±ez\n","\n","- **Usuario o nombre de equipo en Codalab:** Equipo7\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YKrugJB0Xqzu","colab_type":"text"},"source":["----------------------------------------"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-31T13:34:25.683540Z","start_time":"2020-03-31T13:34:25.673430Z"},"id":"UxghL2_2Xqzv","colab_type":"text"},"source":["## 1. Introducci√≥n"]},{"cell_type":"markdown","metadata":{"id":"oIU_sRcUXqzx","colab_type":"text"},"source":["En el presente informe se registra el trabajo realizado para el An√°lisis de Sentimientos sobre datos (tweets) de la red social Twitter. La tarea a abordar consiste en clasificar cuatro sentimientos: enojo, miedo, alegr√≠a y tristeza; seg√∫n tres niveles de intensidad: bajo, medio y alto. En espec√≠fico, se pretende construir, a partir de herramientas de aprendizaje de m√°quinas, cuatro clasificadores que puedan abordar esta tarea para cada uno de los sentimientos antes mencionados. Para esto, el equipo de desarrollo realiza varios experimentos con tal de llegar a clasificadores que obtengan resultados que mejoren el baseline entregado por el equipo docente y a partir del cual se empieza a construir. Para construir estos clasificadores, los tweets se representan de varias maneras, capturando caracter√≠sticas que estos tengan. Entre estas representaciones se incluyen el n√∫mero de caracteres especiales o emoticones, el n√∫mero de palabras alargadas y vectores obtenidos de Word Embeddings pre-entrenados para representar de manera densa los tweets. A su vez, se utilizan los m√©todos Tf-idf y Bag of Words, que permiten representar los documentos seg√∫n la frecuencia de aparici√≥n de palabras. Por otro lado, para clasificar se utilizan tres algoritmos: Support Vector Machine, Logistic Regression y Naive Bayes. Como principal conclusi√≥n luego de realizado este trabajo, se tiene que un mayor n√∫mero de representaciones o caracter√≠sticas para resolver una tarea de clasificaci√≥n no necesariamente genera mejores resultados. √âstos se obtienen escogiendo las caracter√≠sticas de manera inteligente, mediante muchas pruebas de combinaciones, convergiendo, de esta manera, a un resultado √≥ptimo."]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-31T13:47:13.474238Z","start_time":"2020-03-31T13:47:13.454068Z"},"id":"67aR_mTwXqzz","colab_type":"text"},"source":["## 2. Representaciones"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-31T13:47:17.719268Z","start_time":"2020-03-31T13:47:17.709207Z"},"id":"PE-TYx6iXqz0","colab_type":"text"},"source":["A continuaci√≥n se detallan las representaciones utilizadas para la realizaci√≥n del problema.\n","\n","**Bag of words**: permite representar el tweet seg√∫n la frecuencia de aparici√≥n de palabras en un documento. Esto se representa como un vector, donde cada dimensi√≥n de √©l es una palabra perteneciente al vocabulario construido sobre todo los documentos analizados.\n","\n","**Tf-idf**: de las siglas en ingl√©s term frequency-inverse document frequency, es una forma de representar un documento seg√∫n qu√© tan importante es una palabra (term) dentro de un documento, tomando como referencia la colecci√≥n total de los documentos. \n","\n","**Cuenta de emoticones**: dado que un emotic√≥n puede tener una estrecha relaci√≥n con el sentimiento de una oraci√≥n en la comunicaci√≥n escrita moderna, se decide contar la frecuencia de emoticones. Para esto, se define un conjunto de emoticones que est√©n relacionados con cada uno de los sentimientos a analizar y se cuenta en cada tweet cu√°ntos hay presentes de cada uno de estos conjuntos.\n","\n","**Cuenta de palabras alargadas**: se cree que el uso de palabras alargadas puede representar mayor intensidad en lo que se desea expresar. Es por esto que una de la caracter√≠sticas utilizadas es contar cu√°ntas palabras alargadas hay presentes en un tweet. Es importante destacar que s√≥lo se se cuentan como palabras alargadas aquellas que presentan un mismo caracter repetido al menos tres veces. \n","\n","**Vader**: es una librer√≠a espcializada en an√°lisis de sentimientos. La principal funci√≥n que tiene es poder evaluar una oraci√≥n, otorg√°ndole un puntaje seg√∫n si es positiva, neutra o negativa. Se usa como representaci√≥n el puntaje obtenido para un tweet.\n","\n","**Word Embedding**: una forma m√°s sofisticada de representar documentos es  mediante el uso de Word Embeddings. Estos permiten representar documentos como vectores, tal cual lo hace Bag of Words, pero se hace cargo de la deficiencia de este √∫ltimo que la similitud de dos vectores no significa nada para el modelo. A su vez, permite disminuir la dimensionalidad de la representaci√≥n. En este trabajo se utilizan dos Word Embeddings pre-entrenados: twitter-glove-25 y twitter-glove-50, que fueron construidos con tweets y tienen 25 y 50 columnas, respectivamente.\n","\n","**N-gramas**: si bien Bag of Words y Tf-idf utilizan una palabra por defecto para representar un documento, existe la posibilidad de utilizar conjuntos de palabras. Para el caso de este problema, se decide utilizar bigramas es decir, contar la aparici√≥n de conjuntos de 2 palabras.\n","\n","Para construir los experimentos se utilizan distintas combinaciones de estas representaciones. Para emplear algunas de estas caracter√≠sticas, se crea una clase Transformer para poder usarlas a trav√©s de un Pipeline.  \n"]},{"cell_type":"markdown","metadata":{"id":"3y5QVZD6Xqz1","colab_type":"text"},"source":["## 3. Algoritmos"]},{"cell_type":"markdown","metadata":{"id":"uxfKdPGhXqz2","colab_type":"text"},"source":["Los algoritmos de clasificaci√≥n usados fueron SVC, Regresi√≥n Log√≠stica, Naive Bayes.\n","\n","Naive Bayes: el clasificador multinomial Naives Bayes se basa en aplicar el teorema de Bayes con fuertes supuestos de independencia. Dado un vector $x=(x_{1},...,x_{n})$ representado n variables independientes en este caso documentos, le asigna la probabilidad\n","\n","$p(C_{k}|x_{1},...,x_{n})$\n","\n","para cada clase $C_{k}$. Para simplificar esta probabilidad, se asume independencia entre las variables, permitiendo el uso del teorema de Bayes. De esta manera se puede descomponer la probabilidad como:\n","\n","$p(C_{k}|x)=\\frac{p(C_{k})p(x|C_{k})}{p(x)}$\n","\n","El denominador es constante, ya que es la probabilidad del documento dado el total de documentos. \n","El numerador, por su parte, es equivalente a la probabilidad conjunta de $p(C_{k},x_{1},...,x_{n})$, y con el supuesto de independencia llegamos a que eso es proporcional a $p(C_{k})\\prod_{i=1}^{n}p(x_{i}|C_{k})$.\n","\n","Dado esto, lo que el clasificador busca es encontrar la clase m√°s probable, para lo cual maximiza la probabilidad antes mencionada con la siguiente funci√≥n:\n","\n","$y=argmax_{k\\in\\{1,...,K\\}}p(C_{k})\\prod_{i=1}^{n}p(x_{i}|C_{k})$.\n","\n","Regresi√≥n Log√≠stica: forma parte del conjunto de modelos lineales. Es un algoritmo de clasificaci√≥n que predice la probabilidad de una variable dependiente de una clase. Dicha variable dependiente es una variable binaria que es 1 cuando es un √©xito o un 0 cuando es un fallo. La regresi√≥n log√≠stica predice la $P(Y=1)$ como una funci√≥n de $X$. La funci√≥n log√≠stica se puede definir como el logaritmo natural del cuociente entre los eventos de √©xito y de fracaso. Para predecir la probabilidad buscada se usa la funci√≥n sigmoide que se define como la inversa de la funci√≥n log√≠stica:\n","\n","$\\phi(z) = \\frac{1}{1+e^{-z}}$\n","\n","Luego se define una funci√≥n Loss que representa el error del modelo, y utilizando el m√©todo del gradiente descendiente, se puede minimizar iterativamente esta funci√≥n hasta llegar a un m√≠nimo local.\n","\n","SVC: Support Vector Classifier es un clasificador de la librer√≠a sklearn. Este clasificador est√° basado en el algoritmo de Support Vector Machine. Si bien, este algoritmo no es de naturaleza probabil√≠stica, existe una adpataci√≥n que le da esa caracter√≠stica, que a su vez la librer√≠a implementa. Este algoritmo tiene una interpretaci√≥n un poco m√°s geom√©trica, el cual busca un hiperplano que pueda separar de forma √≥ptima los puntos en las distintas categor√≠as del modelo. \n"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-31T13:47:52.064631Z","start_time":"2020-03-31T13:47:52.044451Z"},"id":"F_YUJbfcXqz4","colab_type":"text"},"source":["## 4. M√©tricas de Evaluaci√≥n"]},{"cell_type":"markdown","metadata":{"id":"NXoj20HhXqz6","colab_type":"text"},"source":["**AUC:** la sigla significa *√Årea bajo la curva*, esta m√©trica calcula el √°rea bajo la curva ROC usando la regla del trapezoide.\n","\n","La curva ROC mencionada (curva de caracter√≠stica operativa del receptor) es un gr√°fico que muestra el rendimiento de un modelo de clasificaci√≥n en todos los umbrales de clasificaci√≥n. Esta curva representa dos par√°metros: Recall y Precision (detallados m√°s adelante).\n","\n","Una forma de interpretar el AUC es considerarlo un estimado de la probabilidad que un clasificador rankee m√°s alto una instancia positiva aleatoriamente  que una instancia negativa aleatoriamente.\n","\n","Su valor se encuentra entre 0 y 1. Dando 0 para un modelo sin predicciones correctas y dando 1 para un modelo de predicci√≥n perfecto.\n","\n","\n","**Kappa:** es una m√©trica que compara la exactitud observada del sistema que se quiere medir con la exactitud de un sistema aleatorio que se considera una exactitud esperada.\n","Se define de la siguiente manera: \n","\n","\n","$kappa = \\frac{TotalAccuracy - RandomAccuracy}{1-RandomAccuracy} $\n","\n","\n","Donde TotalAccuracy es la suma de los aciertos sobre el total:\n","\n","\n","$TotalAccuracy = \\frac{TruePos + TrueNeg}{TruePos + TrueNeg + FalsePos +FalseNeg}$\n","\n","\n","Y Random Accuracy se define como la suma de los productos de la probabilidad de referencia y la probabilidad resultante sobre el cuadrado del total:\n","\n","\n","$RandomAccuracy = \\frac{(TrueNeg+FalsePos)*(TrueNeg+FalseNeg)+(TruePos+FalseNeg)*(TruePos+FalsePos)}{Total^2}$\n","\n","\n","\n","Kappa siempre es menor a 1. Aunque no hay una medida est√°ndar para interpretar sus valores, Landis y Koch (1977) indicaron una manera de caracterizar sus valores. De acuerdo a su esquema, un valor menor o igual a cero indica que es un clasificador in√∫til, entre 0 y 0.20 un resultado ligero, entre 0.21 y 0.40 un resultado decente, entre 0.41-0.60 resultado moderado, 0.61- 0.80 resultado sustancial y 0.81-1 un resultado practicamente perfecto. \n","\n","\n","\n","**Accuracy:** Esta m√©trica es la suma de los aciertos sobre el total, que indica qu√© fracci√≥n de clasificaciones del sistema son correctas, tal como se mencion√≥ anteriormente y se describe as√≠:\n","\n","$Accuracy = \\frac{TruePos + TrueNeg}{TruePos + TrueNeg + FalsePos +FalseNeg}$\n","\n","Es la clasificaci√≥n m√°s intuitiva ya que indica el porcentaje de aciertos en el total de casos a clasificar. A partir de aqu√≠ se desprenden tres mediciones que entregan distinta informaci√≥n.\n","\n","Recall o sensibilidad es la fracci√≥n de instancias que han sido recuperadas y se describe de la siguiente manera:\n","\n","$Recall = \\frac{TruePos }{TruePos +FalseNeg}$\n","\n","Precision es la fracci√≥n de instancias que fueron correctamente identificadas, es decir  y se describe as√≠:\n","\n","$Precision = \\frac{TruePos }{TruePos +FalsePos}$\n","\n","Existe un *tradeoff* entre ambas medidas y por eso se puede ponderar ambas en un nuevo valor llamado F1-Score que es la media arm√≥nica entre Precision y Recall. Dando igual importancia a ambos la forma del F1-Score es la siguiente:\n","\n","\n","$F_{1}=\\frac{2* Precision* Recall}{Precision+Recall}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MFdDi8b0Xqz6","colab_type":"text"},"source":["## 5. Experimentos"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-31T13:31:40.023344Z","start_time":"2020-03-31T13:31:40.003541Z"},"id":"8l_sM9HPXqz9","colab_type":"text"},"source":["### Importar librer√≠as y utiles"]},{"cell_type":"code","metadata":{"id":"xDMZpoyDPDiv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":402},"outputId":"9416400b-a67f-430e-871c-9e612f960cb6"},"source":["# Para instalar de ser necesario\n","!pip install emoji\n","!pip install regex\n","!pip install vaderSentiment"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n","\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 2.0MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=d90127c96ac0d3bc223042f0507d7d4a7f5746ee87deb250c27d0301dde521fa\n","  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.5.4\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n","Collecting vaderSentiment\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133kB 3.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vaderSentiment) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2020.4.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (3.0.4)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:20.587160Z","start_time":"2020-04-07T15:44:19.319386Z"},"id":"CyTQF6ItXqz-","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import os\n","import numpy as np\n","import shutil\n","import nltk\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.base import BaseEstimator, TransformerMixin"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d8eutJgzXq0G","colab_type":"text"},"source":["### Definir m√©todos de evaluaci√≥n\n","\n","Estas funciones est√°n a cargo de evaluar los resultados de la tarea. \n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:20.604066Z","start_time":"2020-04-07T15:44:20.589106Z"},"id":"QrcfsMTIXq0K","colab_type":"code","colab":{}},"source":["def auc_score(test_set, predicted_set):\n","    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n","    medium_predicted = np.array(\n","        [prediction[1] for prediction in predicted_set])\n","    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n","    high_test = np.where(test_set == 'high', 1.0, 0.0)\n","    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n","    low_test = np.where(test_set == 'low', 1.0, 0.0)\n","    auc_high = roc_auc_score(high_test, high_predicted)\n","    auc_med = roc_auc_score(medium_test, medium_predicted)\n","    auc_low = roc_auc_score(low_test, low_predicted)\n","    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n","             high_test.sum() * auc_high) / (\n","                 low_test.sum() + medium_test.sum() + high_test.sum())\n","    return auc_w\n","\n","\n","def evaulate(predicted_probabilities, y_test, labels, dataset_name):\n","    # Importante: al transformar los arreglos de probabilidad a clases,\n","    # entregar el arreglo de clases aprendido por el clasificador.\n","    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n","    predicted_labels = [\n","        labels[np.argmax(item)] for item in predicted_probabilities\n","    ]\n","    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n","    print(\n","        confusion_matrix(y_test,\n","                         predicted_labels,\n","                         labels=['low', 'medium', 'high']))\n","\n","    print('\\nClassification Report:\\n')\n","    print(\n","        classification_report(y_test,\n","                              predicted_labels,\n","                              labels=['low', 'medium', 'high']))\n","    # Reorder predicted probabilities array.\n","    labels = labels.tolist()\n","    predicted_probabilities = predicted_probabilities[:, [\n","        labels.index('low'),\n","        labels.index('medium'),\n","        labels.index('high')\n","    ]]\n","    auc = round(auc_score(y_test, predicted_probabilities), 3)\n","    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n","    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n","    print(\"Kappa:\", kappa, end='\\t')\n","    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n","    print(\"Accuracy:\", accuracy)\n","    print('------------------------------------------------------\\n')\n","    return np.array([auc, kappa, accuracy])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tz5VTs_gXq0Q","colab_type":"text"},"source":["### Datos\n","\n","Se obtienen los datasets desde el github del curso.\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.068137Z","start_time":"2020-04-07T15:44:20.606061Z"},"id":"12vi557SXq0R","colab_type":"code","colab":{}},"source":["# Datasets de entrenamiento.\n","train = {\n","    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n","    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n","    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n","    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n","}\n","# Datasets que deber√°n predecir para la competencia.\n","target = {\n","    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n","    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n","    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n","    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U5g_f7ciXq0d","colab_type":"text"},"source":["### Analizar los datos \n","\n","Se imprime la cantidad de tweets de cada dataset, seg√∫n su intensidad de sentimiento. Notar que las clases est√°n desbalanceadas. "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.117633Z","start_time":"2020-04-07T15:44:21.090703Z"},"id":"YQnCzq11Xq0d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":573},"outputId":"c4d2bc6a-47a3-4d12-e34d-d584eae24759"},"source":["def get_group_dist(group_name, train):\n","    print(group_name, \"\\n\",\n","          train[group_name].groupby('sentiment_intensity').count(),\n","          '\\n---------------------------------------\\n')\n","for dataset_name in train:\n","    get_group_dist(dataset_name, train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["anger \n","                       id  tweet  class\n","sentiment_intensity                   \n","high                 163    163    163\n","low                  161    161    161\n","medium               617    617    617 \n","---------------------------------------\n","\n","fear \n","                       id  tweet  class\n","sentiment_intensity                   \n","high                 270    270    270\n","low                  288    288    288\n","medium               699    699    699 \n","---------------------------------------\n","\n","joy \n","                       id  tweet  class\n","sentiment_intensity                   \n","high                 195    195    195\n","low                  219    219    219\n","medium               488    488    488 \n","---------------------------------------\n","\n","sadness \n","                       id  tweet  class\n","sentiment_intensity                   \n","high                 197    197    197\n","low                  210    210    210\n","medium               453    453    453 \n","---------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KFQH7W89nXvR","colab_type":"text"},"source":["### Balanceo de clases\n","\n","Para obtener mejores resultados se balancean las clases en los distintos conjuntos de entrenamiento. En este caso se utiliza subsampling, dejando como la cantidad de muestras por categor√≠a como el m√≠nimo encontrado en cada conjunto.\n"]},{"cell_type":"code","metadata":{"id":"KKp3Z0xcacDV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"f43616a1-f713-4f53-d105-54233983ca27"},"source":["min_num_samples_anger = 161\n","min_num_samples_fear = 270\n","min_num_samples_joy = 195\n","min_num_samples_sadness = 197\n","\n","an = train['anger'].groupby('sentiment_intensity')\n","train['anger'] = pd.DataFrame(\n","    an.apply(lambda x: x.sample(min_num_samples_anger).reset_index(drop=True))).reset_index(\n","        drop=True)\n","fe = train['fear'].groupby('sentiment_intensity')\n","train['fear'] = pd.DataFrame(\n","    fe.apply(lambda x: x.sample(min_num_samples_fear).reset_index(drop=True))).reset_index(\n","        drop=True)\n","jo = train['joy'].groupby('sentiment_intensity')\n","train['joy'] = pd.DataFrame(\n","    jo.apply(lambda x: x.sample(min_num_samples_joy).reset_index(drop=True))).reset_index(\n","        drop=True)\n","sa = train['sadness'].groupby('sentiment_intensity')\n","train['sadness'] = pd.DataFrame(\n","    sa.apply(lambda x: x.sample(min_num_samples_sadness).reset_index(drop=True))).reset_index(\n","        drop=True)\n","\n","  \n","train['fear'].sentiment_intensity.value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["medium    270\n","low       270\n","high      270\n","Name: sentiment_intensity, dtype: int64"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"nIfyoxUrXq0l","colab_type":"text"},"source":["### Custom Features \n","\n","Para crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n","\n","1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta car√°cteres relevantes ('!', '?', '#') en los tweets.\n","2. Definios una funci√≥n c√≥mo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n","3. Hacemos un override de la funci√≥n `transform` en donde iteramos por cada tweet, llamamos a la funci√≥n que hicimos antes y agregamos sus resultados a un arrelo. Finalmente lo retornamos.\n","\n","Esto nos facilitar√° el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aqu√≠](https://scikit-learn.org/stable/data_transforms.html).\n","\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.128600Z","start_time":"2020-04-07T15:44:21.119624Z"},"id":"Ev8s-pFjXq0l","colab_type":"code","colab":{}},"source":["\n","import emoji\n","import regex\n","class CharsCountTransformer(BaseEstimator, TransformerMixin):\n","\n","    def get_relevant_chars(self, tweet):\n","        num_hashtags = tweet.count('#')\n","        num_exclamations = tweet.count('!')\n","        num_interrogations = tweet.count('?')\n","        return [num_hashtags, num_exclamations, num_interrogations]\n","\n","    def transform(self, X, y=None):\n","        chars = []\n","        for tweet in X:\n","            chars.append(self.get_relevant_chars(tweet))\n","\n","        return np.array(chars)\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","class EmojiCountTransformer(BaseEstimator, TransformerMixin):\n","    def count_happy_emojis(self, tweet):\n","        count = 0\n","        happy_emojis = \"üòÄüòÉüòÑüòÅüòÜüòÖü§£üòÇüôÇüòäüòâü§©‚ò∫üòãüòåü•≥üòéüò∫üò∏üòπüôâ\"\n","        for c in tweet:\n","            if c in happy_emojis:\n","                count += 1\n","\n","        return count\n","\n","    def count_sad_emojis(self, tweet):\n","        count = 0\n","        sad_emojis = \"üòîüò™üò∑ü§ïü§¢üòüüôÅ‚òπü•∫üò¢üò≠üòûüòø\"\n","        for c in tweet:\n","            if c in sad_emojis:\n","                count += 1\n","\n","        return count\n","\n","    def count_fear_emojis(self, tweet):\n","        count = 0\n","        fear_emojis = \"üòêüò∂üòüüò¶üòßüò∞üò•üò±üò©üò´üôÄ\"\n","        for c in tweet:\n","            if c in fear_emojis:\n","                count += 1\n","\n","        return count\n","\n","    def count_anger_emojis(self, tweet):\n","        count = 0\n","        anger_emojis = \"ü§îüòëüòíüò°üò†ü§¨üëøüòæ\"\n","        for c in tweet:\n","            if c in anger_emojis:\n","                count += 1\n","\n","        return count\n","    \n","    def get_relevant_emojis(self, tweet):\n","        num_happy = self.count_happy_emojis(tweet)\n","        num_sad = self.count_sad_emojis(tweet)\n","        num_fear = self.count_fear_emojis(tweet)\n","        num_anger = self.count_anger_emojis(tweet)\n","        return [num_happy, num_sad, num_anger, num_anger]\n","    \n","    def transform(self, X, y=None):\n","        emojis = []\n","        for tweet in X:\n","            emojis.append(self.get_relevant_emojis(tweet))\n","\n","        return np.array(emojis)\n","\n","    def fit(self, X, y=None):\n","        return self\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3px7KvhI77CH","colab_type":"code","colab":{}},"source":["import string\n","class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n","    \"\"\" Transforma tweets a representaciones vectoriales usando alg√∫n modelo de Word Embeddings.\n","    \"\"\"\n","    \n","    def __init__(self, model, aggregation_func):\n","        self.model = model\n","        \n","        self.aggregation_func = aggregation_func\n","\n","    def simple_tokenizer(self, doc, lower=False):\n","        \"\"\"Tokenizador. Elimina signos de puntuaci√≥n, lleva las letras a min√∫scula(opcional) y \n","           separa el tweet por espacios.\n","        \"\"\"\n","        if lower:\n","            doc.translate(str.maketrans('', '', string.punctuation)).lower().split()\n","        return doc.translate(str.maketrans('', '', string.punctuation)).split()\n","\n","    def transform(self, X, y=None):\n","        \n","        doc_embeddings = []\n","        \n","        for doc in X:\n","            tokens = self.simple_tokenizer(doc, lower = True) \n","            \n","            selected_wv = []\n","            for token in tokens:\n","                if token in self.model.vocab:\n","                    selected_wv.append(self.model[token])\n","                    \n","            # si seleccionamos por lo menos un embedding para el tweet, lo agregamos y luego lo a√±adimos.\n","            if len(selected_wv) > 0:\n","                doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n","                doc_embeddings.append(doc_embedding)\n","            # si no, a√±adimos un vector de ceros que represente a ese documento.\n","            else:\n","                doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo \n","\n","        return np.array(doc_embeddings)\n","\n","    def fit(self, X, y=None):\n","        return self"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYHEln83jDjk","colab_type":"code","colab":{}},"source":["class VaderPolarityTransformer(BaseEstimator, TransformerMixin):\n","    def __init__(self, analyzer):\n","        self.analyzer = analyzer\n","\n","    def get_polarities(self, doc):\n","        score = self.analyzer.polarity_scores(doc)\n","        return [score['pos'], score['neu'], score['neg'], score['compound']]\n","\n","    def transform(self, X, y=None):\n","        polarities = []\n","        for tweet in X:\n","            polarities.append(self.get_polarities(tweet))\n","\n","        return np.array(polarities)\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f2RaQKQotpc5","colab_type":"code","colab":{}},"source":["import re\n","class ElongatedWordsAndPunctuationsTransformer(BaseEstimator, TransformerMixin):\n","    def get_elongated_words_count(self, tokens):\n","        count = 0\n","        elong = re.compile(\"([a-zA-Z])\\\\1{2,}\")\n","        for token in tokens:\n","            if elong.search(token) != None:\n","                count += 1\n","        return count\n","\n","    def get_punctuation_marks(self, tokens):\n","        count = 0\n","        elong = re.compile(\"([,.¬°!¬ø?])\\\\1{1,}\")\n","        for token in tokens:\n","            if elong.search(token)!=None:\n","                count += 1\n","        return count\n","\n","    def get_elongated_count(self, doc):\n","        elongated_words = self.get_elongated_words_count(doc.split())\n","        punctuation_marks = self.get_punctuation_marks(doc.split())\n","        return [elongated_words, punctuation_marks]\n","\n","    def transform(self, X, y=None):\n","        marks = []\n","        for tweet in X:\n","            marks.append(self.get_elongated_count(tweet))\n","\n","        return np.array(marks)\n","\n","    def fit(self, X, y=None):\n","        return self"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"83Oq1mTBXq0u","colab_type":"text"},"source":["### Definir la representaci√≥n y el clasificador\n","\n","Para esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando as√≠ nuestra programaci√≥n.\n","\n","El pipeline m√°s b√°sico que podemos hacer es transformar el dataset a Bag of Words y despu√©s usar clasificar el BoW usando NaiveBayes:\n","\n","```python\n","    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n","```\n","\n","\n","Ahora, si queremos usar nuestra transformaci√≥n para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenar√° los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n","\n","```python\n","    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n","                                        ('chars_count',CharsCountTransformer())])),\n","              ('clf', MultinomialNB())])\n","\n","```\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FrsgAq1pXq0w","colab_type":"text"},"source":["Recuerden que cada pipeline representa un sistema de clasificaci√≥n distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podr√≠an solapar resultados.  Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:"]},{"cell_type":"code","metadata":{"id":"2Uq_CiCT-YYj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"dfeb5b9d-10f1-4ecd-81b7-63cc16c624f4"},"source":["import gensim.downloader as api\n","# Se sugiera correr esta celda una vez pues toma tiempo descargar los modelos\n","model = api.load(\"glove-twitter-25\")  \n","model2 = api.load(\"glove-twitter-50\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[=================================-----------------] 66.4% 69.6/104.8MB downloaded\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["[================================================--] 96.4% 192.4/199.5MB downloaded\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zvET_n3k9CQv","colab_type":"text"},"source":["### Resumen de experimentos\n","\n","Los experimentos consisten en hacer combinaciones de las representaciones mencionadas en esa secci√≥n del informe m√°s el uso de alguno de los algoritmos de clasificaci√≥n. A continuaci√≥n se detallan aspectos generales que marcan la diferencia a la hora de comparar los resultados de los experimentos:\n","\n","**Algoritmo escogido**: si bien se utilizaron 3 algoritmos de clasificaci√≥n, el clasificador basado en Regresi√≥n Log√≠stica fue el que obtuvo mejores resultados. Si bien Naive Bayes muestra resultados aceptables, al tener su supuesto de independencia y basado en vectores de frecuencia, no es compatible con otras caracter√≠sticas como Word Embedding o los puntajes otorgados por Vader. Por otro lado, SVC no es un algoritmo de origen probabil√≠stico, por lo que tal vez para este problema, aun cuando est√° adaptado para arrojar probabilidades, no lo hace de manera eficaz.\n","\n","**Librer√≠a Vader**: se evidencia un mejora de resultados al utilizar la librer√≠a Vader. En particular, se registran los mejores resultados en la felicidad y tristeza. Al no utilizar esta librer√≠a se visualiza una peque√±a baja en las m√©tricas.\n","\n","**Word Embedding**: por s√≠ solo, utilizar Word Embedding es mejor que utilizar alguna de las otras 2 representaciones vectoriales (Bag of Words o Tf-idf). No se muestran diferencias sustanciales entre usar el modelo de 25 o 50 dimensiones, por lo que se prefiere el de 25. El Transformer que se utiliza (extra√≠do de la Clase Auxiliar 2) requiere de una funci√≥n de agregaci√≥n (que permitiera unir o representar en 1 vector los vectores de todos las palabras de un tweet). En este caso, la funci√≥n que mejores resultados obtiene es la del promedio de cada dimensi√≥n. Finalmente, no se usa esta representaci√≥n por s√≠ sola. Mediante los experimentos se evidencia que en conjunto con el vectorizador Tf-idf se obtienen mejores resultados.\n","\n","**Tf-idf**: el uso del vectorizador de Tf-idf muestra mejores resultados que el uso de Bag of Words, pero no mejores que con el uso del Word Embedding.\n","\n","Uso de emoticones, palabras alargadas no mostraron mayores diferencias en los resultados. Aun as√≠, se incluyeron en varios de los experimentos.\n","\n","Para poder comparar resultados cada experimento se realiza diez veces y sus datos son recogidos con tal de generar una mediana y un promedio por cada emoci√≥n y para cada m√©trica. A continuaci√≥n se detallan los clasificadores y la combinaci√≥n de caracter√≠sticas junto con el promedio general de resultados obtenidos por cada experimento.\n","\n","\n","### Experimentos realizados\n","| Resumen de experimentos | Caracter√≠sticas                                                                                                                                                                                                                                                                                                           | Clasificador                                  | AUC Promedio | Kappa Promedio | Accuracy Promedio |\n","|-------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|--------------|----------------|-------------------|\n","| 1                       | Vector de Tf-idf.                                                                                                                                                                                                                                                                                                         | Naive Bayes                                   |       0.7293 |         0.3044 |            0.5352 |\n","| 2                       | Vector de Bag of Words, frecuencia de caracteres especiales, <br>frecuencia de emoticones.                                                                                                                                                                                                                                | Regresi√≥n Log√≠stica                           |       0.6445 |         0.1895 |            0.4509 |\n","| 3                       | Vector del promedio de los vectores de las palabras seg√∫n el<br>Word Embedding de 25 dimensiones, puntajes de Vader, frecuencia<br>de caracteres especiales, frecuencia de emoticones, frecuencia<br>de palabras alargadas.                                                                                               | Regresi√≥n Log√≠stica                           |       0.7551 |         0.3339 |            0.5581 |\n","| 4                       | Igual que experimento 3 m√°s vector de Bag of Words.                                                                                                                                                                                                                                                                       | Regresi√≥n Log√≠stica                           |       0.7757 |         0.3869 |            0.5906 |\n","| 5                       | Puntajes de Vader.                                                                                                                                                                                                                                                                                                        | Regresi√≥n Log√≠stica                           |       0.7158 |         0.3216 |            0.5452 |\n","| 6                       | Igual que experimento 4 salvo que se usa vector de Tf-idf en vez<br>de Bag of Words.                                                                                                                                                                                                                                      | Regresi√≥n Log√≠stica                                           |       0.7771 |         0.3957 |            0.5954 |\n","| 7                       | Vector de Bag of Words, vector resultante del promedio de cada<br>dimensi√≥n de vectores de las palabras seg√∫n el Word Embedding<br>de 25 dimensiones, puntajes de Vader, frecuencia de caracteres <br>especiales, frecuencia de emoticones, frecuencia de palabras <br>alargadas.                                         | SVC                           |       0.7534 |   0.3325555556 |            0.5558 |\n","| 8                       | Igual que 6 salvo que se usa funci√≥n de m√°ximo en agregaci√≥n <br>de vectores de Word Embedding.                                                                                                                                                                                                                           | Regresi√≥n Log√≠stica                           |       0.7631 |   0.3593333333 |            0.5731 |\n","| 9                       | Igual que 6 salvo que se usa funci√≥n de suma en agregaci√≥n <br>de vectores de Word Embedding                                                                                                                                                                                                                              | Regresi√≥n Log√≠stica                           |        0.773 |          0.385 |            0.5898 |\n","| 10                      | Vector de Tf-idf considerando bigramas, vector de Bag of Words, <br>vector resultante de la suma de cada dimensi√≥n de vectores de <br>las palabras seg√∫n el Word Embedding de 25 dimensiones, puntajes<br>de Vader, frecuencia de caracteres especiales, frecuencia de <br>emoticones, frecuencia de palabras alargadas.  | Regresi√≥n Log√≠stica                           |        0.768 |   0.3673333333 |            0.5791 |\n","| 11                      | Vector de Tf-idf considerando bigramas, vector de Bag of Words, <br>vector resultante de la suma de cada dimensi√≥n de vectores de <br>las palabras seg√∫n el Word Embedding de 25 dimensiones, puntajes <br>de Vader, frecuencia de caracteres especiales, frecuencia de <br>emoticones, frecuencia de palabras alargadas. | Regresi√≥n Log√≠stica                           |       0.7762 |   0.3866666667 |             0.589 |\n","| 12                      | Vector de Tf-idf, vector resultante de la suma de cada dimensi√≥n <br>de vectores de las palabras seg√∫n el Word Embedding de 25 <br>dimensiones, puntajes de Vader.                                                                                                                                                        | Regresi√≥n Log√≠stica                           |       0.7776 |   0.3866666667 |            0.5898 |\n","| 13                      | Vector de Tf-idf.                                                                                                                                                                                                                                                                                                         | Regresi√≥n Log√≠stica                           |       0.7244 |   0.3074444444 |             0.531 |\n","| 14                      | Vector de Tf-idf y vector de Word Embedding con funci√≥n del <br>promedio.                                                                                                                                                                                                                                                 | Regresi√≥n Log√≠stica                           |       0.7255 |   0.2953333333 |            0.5286 |\n","| 15                      | Igual que experimento 6.                                                                                                                                                                                                                                                                                                  | Regresi√≥n Log√≠stica <br>(2000000 iteraciones) |       0.7731 |           0.38 |            0.5827 |\n","| 16                      | Igual que experimento 6.                                                                                                                                                                                                                                                                                                  | Regresi√≥n Log√≠stica <br>(1000 iteraciones)    |       0.7768 |   0.3894444444 |            0.5941 |\n","| 17                      | Igual que experimento 6 salvo que se us√≥ modelo de Word <br>Embedding de vectores de 50 dimensiones.                                                                                                                                                                                                                      | Regresi√≥n Log√≠stica                           |       0.7753 |   0.3823333333 |            0.5855 |"]},{"cell_type":"markdown","metadata":{"id":"X-2YLDYHZV_N","colab_type":"text"},"source":["En seguida se presenta un resumen de los mejores resultados por emoci√≥n para cada m√©trica donde EX representa el n√∫mero de experimento que obtuvo el el resultado"]},{"cell_type":"markdown","metadata":{"id":"bo3DJhosXIVI","colab_type":"text"},"source":["| Mejores Resultados |              |    |                |    |                   |       |\n","|--------------------|--------------|----|----------------|----|-------------------|-------|\n","|                    |              |    |                |    |                   |       |\n","| Resumen Promedios  | AUC Promedio | EX | Kappa Promedio | EX | Accuracy Promedio | EX    |\n","| Promedios Anger    |       0.7681 |  4 |          0.379 |  4 |            0.5863 |     4 |\n","| Promedios Fear     |       0.7835 | 12 |         0.3992 | 12 |            0.5992 |    12 |\n","| Promedios Joy      |       0.8183 | 10 |         0.4638 | 10 |            0.6422 |    10 |\n","| Promedios Sadness  |       0.7721 |  6 |         0.3807 |  6 |            0.5872 |     6 |\n","| Promedios Promedio |       0.7776 | 12 |         0.3957 |  6 |            0.5954 |     6 |\n","|                    |              |    |                |    |                   |       |\n","|                    |              |    |                |    |                   |       |\n","| Resumen Medias     | AUC Promedio | EX | Kappa Promedio | EX | Accuracy Promedio | EX    |\n","| Media Anger        |        0.772 |  4 |         0.3795 | 16 |            0.5875 |    16 |\n","| Media Fear         |       0.7805 | 12 |         0.4095 | 12 |             0.606 |    12 |\n","| Media Joy          |        0.823 | 10 |         0.4755 |  9 |             0.644 | 10-17 |\n","| Media Sadness      |       0.7805 |  6 |         0.3805 | 17 |             0.587 |    17 |\n","| Media Promedio     |        0.778 | 17 |         0.3985 |  6 |            0.5985 |    16 |"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.155528Z","start_time":"2020-04-07T15:44:21.149545Z"},"id":"cuulTk71Xq0x","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","analyzer = SentimentIntensityAnalyzer()\n","\n","def get_experiment_0_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('bow', CountVectorizer()),\n","                                    ('chars_count', CharsCountTransformer())\n","                                    ])), ('clf', MultinomialNB())])\n","\n","def get_experiment_1_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer())\n","                                    ])), ('clf', MultinomialNB())])\n","\n","def get_experiment_2_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('bow', CountVectorizer(ngram_range=(3,3))),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer())\n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","\n","\n","\n","def get_experiment_3_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","\n","\n","def get_experiment_4_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('bow', CountVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","    \n","def get_experiment_5_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('vader', VaderPolarityTransformer(analyzer))\n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","\n","\n","# el mejor hasta ahora\n","def get_experiment_6_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","\n","def get_experiment_7_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', SVC(probability=True))])\n","    \n","def get_experiment_8_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.max)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_9_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.sum)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_10_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer(ngram_range=(2,2))),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_11_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer(ngram_range=(1,2))),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_12_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer))\n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_13_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer())\n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_14_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean))\n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_15_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=2000000))])\n","    \n","def get_experiment_16_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000))])\n","\n","def get_experiment_17_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model2, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4eL9AYayXq05","colab_type":"text"},"source":["### Ejecutar el pipeline para alg√∫n dataset"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.167498Z","start_time":"2020-04-07T15:44:21.157540Z"},"scrolled":true,"id":"fS1vWn2mXq05","colab_type":"code","colab":{}},"source":["def run(dataset, dataset_name, pipeline):\n","    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n","    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n","\n","    # Dividimos el dataset en train y test.\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        dataset.tweet,\n","        dataset.sentiment_intensity,\n","        shuffle=True,\n","        test_size=0.33)\n","\n","\n","    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n","    pipeline.fit(X_train, y_train)\n","\n","    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n","    predicted_probabilities = pipeline.predict_proba(X_test)\n","\n","    # Obtenemos el orden de las clases aprendidas.\n","    learned_labels = pipeline.classes_\n","\n","    # Evaluamos:\n","    scores = evaulate(predicted_probabilities, y_test, learned_labels, dataset_name)\n","    return pipeline, learned_labels, scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCh7ffVtXq08","colab_type":"text"},"source":["### Ejecutar el sistema creado por cada train set\n","\n","Este c√≥digo crea y entrena los 4 sistemas de clasificaci√≥n y luego los evalua. Para los experimentos, pueden copiar este c√≥digo variando el pipeline cuantas veces estimen conveniente."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.384119Z","start_time":"2020-04-07T15:44:21.170488Z"},"scrolled":false,"id":"vxtKeEZ2Xq0-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7d3f7efa-cfd2-432c-c5be-bbeb24221679"},"source":["classifiers = []\n","learned_labels_array = []\n","scores_array = []\n","\n","# Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n","for dataset_name, dataset in train.items():\n","    \n","    # creamos el pipeline\n","    # cambiar seg√∫n experimento a ejecutar\n","    pipeline = get_experiment_15_pipeline()\n","    \n","    # ejecutamos el pipeline sobre el dataset\n","    classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n","\n","    # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n","    classifiers.append(classifier)\n","\n","    # guardamos las labels aprendidas por el clasificador\n","    learned_labels_array.append(learned_labels)\n","\n","    # guardamos los scores obtenidos\n","    scores_array.append(scores)\n","\n","# print avg scores\n","print(\n","    \"Average scores:\\n\\n\",\n","    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n","    .format(*np.array(scores_array).mean(axis=0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Confusion Matrix for anger:\n","\n","[[35 15  7]\n"," [12 20 24]\n"," [ 3  8 36]]\n","\n","Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","         low       0.70      0.61      0.65        57\n","      medium       0.47      0.36      0.40        56\n","        high       0.54      0.77      0.63        47\n","\n","    accuracy                           0.57       160\n","   macro avg       0.57      0.58      0.56       160\n","weighted avg       0.57      0.57      0.56       160\n","\n","Scores:\n","\n","AUC:  0.778\tKappa: 0.358\tAccuracy: 0.569\n","------------------------------------------------------\n","\n","Confusion Matrix for fear:\n","\n","[[59 22  7]\n"," [29 38 25]\n"," [14 20 54]]\n","\n","Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","         low       0.58      0.67      0.62        88\n","      medium       0.47      0.41      0.44        92\n","        high       0.63      0.61      0.62        88\n","\n","    accuracy                           0.56       268\n","   macro avg       0.56      0.57      0.56       268\n","weighted avg       0.56      0.56      0.56       268\n","\n","Scores:\n","\n","AUC:  0.75\tKappa: 0.346\tAccuracy: 0.563\n","------------------------------------------------------\n","\n","Confusion Matrix for joy:\n","\n","[[48 13  3]\n"," [11 39 12]\n"," [ 5 15 48]]\n","\n","Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","         low       0.75      0.75      0.75        64\n","      medium       0.58      0.63      0.60        62\n","        high       0.76      0.71      0.73        68\n","\n","    accuracy                           0.70       194\n","   macro avg       0.70      0.69      0.70       194\n","weighted avg       0.70      0.70      0.70       194\n","\n","Scores:\n","\n","AUC:  0.856\tKappa: 0.544\tAccuracy: 0.696\n","------------------------------------------------------\n","\n","Confusion Matrix for sadness:\n","\n","[[42 17  6]\n"," [25 23 24]\n"," [ 6 11 42]]\n","\n","Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","         low       0.58      0.65      0.61        65\n","      medium       0.45      0.32      0.37        72\n","        high       0.58      0.71      0.64        59\n","\n","    accuracy                           0.55       196\n","   macro avg       0.54      0.56      0.54       196\n","weighted avg       0.53      0.55      0.53       196\n","\n","Scores:\n","\n","AUC:  0.737\tKappa: 0.323\tAccuracy: 0.546\n","------------------------------------------------------\n","\n","Average scores:\n","\n"," Average AUC: 0.78\t Average Kappa: 0.393\t Average Accuracy: 0.593\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2019-08-21T19:37:43.169737Z","start_time":"2019-08-21T19:37:43.166744Z"},"id":"kggQZAa6Xq1I","colab_type":"text"},"source":["### Predecir los target set y crear la submission\n","\n","Aqu√≠ predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.392097Z","start_time":"2020-04-07T15:44:21.386114Z"},"id":"mGmF-dOkXq1J","colab_type":"code","colab":{}},"source":["def predict_target(dataset, classifier, labels):\n","    # Predecir las probabilidades de intensidad de cada elemento del target set.\n","    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n","    # Agregar ids\n","    predicted['id'] = dataset.id.values\n","    # Reordenar las columnas\n","    predicted = predicted[['id', 'low', 'medium', 'high']]\n","    return predicted"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.588573Z","start_time":"2020-04-07T15:44:21.394094Z"},"scrolled":true,"id":"bvhd4VsPXq1N","colab_type":"code","colab":{}},"source":["predicted_target = {}\n","\n","# Crear carpeta ./predictions\n","if (not os.path.exists('./predictions')):\n","    os.mkdir('./predictions')\n","\n","else:\n","    # Eliminar predicciones anteriores:\n","    shutil.rmtree('./predictions')\n","    os.mkdir('./predictions')\n","\n","# por cada target set:\n","for idx, key in enumerate(target):\n","    # Predecirlo\n","    predicted_target[key] = predict_target(target[key], classifiers[idx],\n","                                           learned_labels_array[idx])\n","    # Guardar predicciones en archivos separados. \n","    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n","                                 sep='\\t',\n","                                 header=False,\n","                                 index=False)\n","\n","# Crear archivo zip\n","a = shutil.make_archive('predictions', 'zip', './predictions')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-UHyPIXcXq1R","colab_type":"text"},"source":["## 6. Conclusiones"]},{"cell_type":"markdown","metadata":{"id":"UMOuIeXJXq1S","colab_type":"text"},"source":["Como se puede ver en la secci√≥n de resultados, los mejores experimentos en el resultado general de las tres m√©tricas, seg√∫n promedios, son el n√∫mero 12 en AUC y el 6 en kappa y accuracy. En cuanto al resultado general de la mediana los mejores son el 17 en AUC, el 6 en kappa y el 16 en accuracy. \n"," \n","Lo anterior quiere decir que si se realizan muchos experimentos se puede esperar mejores resultados en promedio para los experimentos 12 y 6. Pero que si se hacen menos experimentos se puede esperar un mejor resultado para los experimentos 17, 6 y 16, ya que tener una mejor mediana significa que son m√°s estables en esa m√©trica.\n"," \n"," \n","Pero si se observan los experimentos m√°s exitosos por sentimiento, en cuanto a promedio, en las tres m√©tricas fueron el n√∫mero 4 para anger, el n√∫mero 12 para fear, el n√∫mero 10 para Joy y el n√∫mero 6 para sadness. En cuanto a mediana de los resultados en las tres m√©tricas, los mejores experimentos son el 4 (AUC) y 16 (kappa y accuracy) para anger, 12 para fear, 10 (AUC y accuracy) 9 (kappa) y 17 (accuracy) para joy, 6 (AUC) y 17 (kappa y accuracy) para sadness. \n","Esta diversidad de experimentos indica que hay combinaciones de caracter√≠sticas que funcionan mejor para un sentimiento determinado. Pero se puede notar que todos los experimentos mencionados comparten el uso de la Regresi√≥n Log√≠stica como clasificador. Adem√°s, que los mejores experimentos (6, 12, 16 y 17) comparten el uso de Tf-idf, Word Embedding y Vader. En particular, el 12 s√≥lo ocupa esas tres cosas obteniendo resultados similares a los dem√°s que tiene m√°s caracter√≠sticas agregadas. El 6 es el experimento base para el 16 y 17 que usan la misma combinaci√≥n de caracter√≠sticas pero con modificaciones en la cantidad de iteraciones en la regresi√≥n lineal o en la cantidad de dimensiones de los vectores de Word Embedding.\n"," \n"," \n","Los diversos experimentos demostraron que m√°s caracter√≠sticas no implica necesariamente mejorar los resultados. Agregar caracter√≠sticas que no proporcionen informaci√≥n relevante puede perjudicar el modelo. Sin embargo, determinar caracter√≠sticas relevantes, es decir que logren entregar informaci√≥n sobre las diferencias entre clases, mejora los resultados.\n"," \n"," \n","Dentro del uso de clasificadores, de los tres que se utilizan, el que entrega mejores resultados es el de Regresi√≥n Lineal, superando a Naive Bayes y SVC.\n"," \n"," \n","Con respecto a el balance entre las clases, el undersampling resulta considerablemente mejor en la pr√°ctica de lo que se espera en teor√≠a. En teor√≠a para clases con menos de mil datos el undersampling puede quitar mucha informaci√≥n valiosa dado que no hay tanto material. Si bien, normalmente se recomienda usar undersampling para clases con decenas de miles de datos, al usarlo se obtuvo mejores resultados en las m√©tricas. De lo que se concluye que se obtiene mejor resultados en las m√©tricas utilizadas con clases balanceadas aunque se use undersampling para clases con menos de mil datos.\n"," \n"," \n","Para un nuevo experimento se propone hacer uso de oversampling sobre los datos de entrenamiento luego de haberlos separado de los datos para testear.\n"," \n"," \n","Tambi√©n se propone usar diferentes combinaciones para cada sentimiento, luego de los resultados obtenidos podemos notar que el usar un clasificador y una combinaci√≥n particular puede entregar excelentes resultados para un sentimiento pero malos resultados para otro. Al detectar los mejores experimentos para cada sentimiento se puede maximizar los resultados obtenidos por cada sentimiento si se crea un pipeline con diferentes caracter√≠sticas y clasificador dependiendo del sentimento.\n"," \n"," \n","Luego de comparar los resultados en la competencia se evidencian buenos resultados en las m√©tricas de AUC y kappa, pero pobres resultados en Accuracy. Durante los experimentos el equipo se enfoca m√°s al uso de herramientas probabil√≠sticas lo que provoca una falta en el uso de diccionarios de palabras relacionadas a los sentimientos, esto se ve reflejado en la baja de puntuaci√≥n en Accuracy. Se propone para mejorar el experimento el uso de sentiment lexicon que puede proporcionar polaridades para distintos conjuntos de palabras relacionadas a cada sentimiento.\n"," \n"," \n","Adem√°s, se nota que los mejores resultados se obtuvieron en el sentimiento Joy, pero pobres resultados en Anger, lo que atribuimos a la clasificaci√≥n que hace Vader sobre si una oraci√≥n es positiva, neutra o negativa. Para un futuro trabajo recomendamos buscar nuevas herramientas para representar mejor los sentimientos con menores resultados, en este caso Anger.\n"," \n"," \n","Finalmente, concluimos que se cumplieron los objetivos, se mejor√≥ los resultados en kappa y AUC del baseline entregado por el equipo docente. Si bien no se mejor√≥ lo suficiente el resultado en accuracy, se realizaron gran diversidad de experimentos para probar diferentes combinaciones de caracter√≠sticas y clasificadores, como se esperaba por parte del equipo docente. Y se propusieron mejoras para trabajos futuros a partir de los resultados obtenidos.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U_pXY-mJohPo","colab_type":"text"},"source":["## Anexos\n","\n","1. Tabla completa de resultados obtenidos por cada experimento con tablas de resumen general y para cada experimento: \n","\n","https://docs.google.com/spreadsheets/d/1ImCuFPfhzCb9LSVKL4aLMPYJDnhzTagkIrmmUKsAZC4/edit?usp=sharing"]},{"cell_type":"code","metadata":{"id":"VodiooLQoWV0","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}