{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"_18767932_assignment_1_f.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-31T13:49:08.174519Z","start_time":"2020-03-31T13:49:08.165989Z"},"id":"DKFrJAgZXqzZ","colab_type":"text"},"source":["# Tarea 1 NLP : Competencia de Clasificación de Texto\n","-------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"WGICQIOfXqzd","colab_type":"text"},"source":["- **Nombre:** Ignacio Díaz Lara, Felipe Manen Núñez\n","\n","- **Usuario o nombre de equipo en Codalab:** Equipo7\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YKrugJB0Xqzu","colab_type":"text"},"source":["----------------------------------------"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-31T13:34:25.683540Z","start_time":"2020-03-31T13:34:25.673430Z"},"id":"UxghL2_2Xqzv","colab_type":"text"},"source":["## 1. Introducción"]},{"cell_type":"markdown","metadata":{"id":"oIU_sRcUXqzx","colab_type":"text"},"source":["En el presente informe se registra el trabajo realizado para el Análisis de Sentimientos sobre datos (tweets) de la red social Twitter. La tarea a abordar consiste en clasificar cuatro sentimientos: enojo, miedo, alegría y tristeza; según tres niveles de intensidad: bajo, medio y alto. En específico, se pretende construir, a partir de herramientas de aprendizaje de máquinas, cuatro clasificadores que puedan abordar esta tarea para cada uno de los sentimientos antes mencionados. Para esto, el equipo de desarrollo realiza varios experimentos con tal de llegar a clasificadores que obtengan resultados que mejoren el baseline entregado por el equipo docente y a partir del cual se empieza a construir. Para construir estos clasificadores, los tweets se representan de varias maneras, capturando características que estos tengan. Entre estas representaciones se incluyen el número de caracteres especiales o emoticones, el número de palabras alargadas y vectores obtenidos de Word Embeddings pre-entrenados para representar de manera densa los tweets. A su vez, se utilizan los métodos Tf-idf y Bag of Words, que permiten representar los documentos según la frecuencia de aparición de palabras. Por otro lado, para clasificar se utilizan tres algoritmos: Support Vector Machine, Logistic Regression y Naive Bayes. Como principal conclusión luego de realizado este trabajo, se tiene que un mayor número de representaciones o características para resolver una tarea de clasificación no necesariamente genera mejores resultados. Éstos se obtienen escogiendo las características de manera inteligente, mediante muchas pruebas de combinaciones, convergiendo, de esta manera, a un resultado óptimo."]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-31T13:47:13.474238Z","start_time":"2020-03-31T13:47:13.454068Z"},"id":"67aR_mTwXqzz","colab_type":"text"},"source":["## 2. Representaciones"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-31T13:47:17.719268Z","start_time":"2020-03-31T13:47:17.709207Z"},"id":"PE-TYx6iXqz0","colab_type":"text"},"source":["A continuación se detallan las representaciones utilizadas para la realización del problema.\n","\n","**Bag of words**: permite representar el tweet según la frecuencia de aparición de palabras en un documento. Esto se representa como un vector, donde cada dimensión de él es una palabra perteneciente al vocabulario construido sobre todo los documentos analizados.\n","\n","**Tf-idf**: de las siglas en inglés term frequency-inverse document frequency, es una forma de representar un documento según qué tan importante es una palabra (term) dentro de un documento, tomando como referencia la colección total de los documentos. \n","\n","**Cuenta de emoticones**: dado que un emoticón puede tener una estrecha relación con el sentimiento de una oración en la comunicación escrita moderna, se decide contar la frecuencia de emoticones. Para esto, se define un conjunto de emoticones que estén relacionados con cada uno de los sentimientos a analizar y se cuenta en cada tweet cuántos hay presentes de cada uno de estos conjuntos.\n","\n","**Cuenta de palabras alargadas**: se cree que el uso de palabras alargadas puede representar mayor intensidad en lo que se desea expresar. Es por esto que una de la características utilizadas es contar cuántas palabras alargadas hay presentes en un tweet. Es importante destacar que sólo se se cuentan como palabras alargadas aquellas que presentan un mismo caracter repetido al menos tres veces. \n","\n","**Vader**: es una librería espcializada en análisis de sentimientos. La principal función que tiene es poder evaluar una oración, otorgándole un puntaje según si es positiva, neutra o negativa. Se usa como representación el puntaje obtenido para un tweet.\n","\n","**Word Embedding**: una forma más sofisticada de representar documentos es  mediante el uso de Word Embeddings. Estos permiten representar documentos como vectores, tal cual lo hace Bag of Words, pero se hace cargo de la deficiencia de este último que la similitud de dos vectores no significa nada para el modelo. A su vez, permite disminuir la dimensionalidad de la representación. En este trabajo se utilizan dos Word Embeddings pre-entrenados: twitter-glove-25 y twitter-glove-50, que fueron construidos con tweets y tienen 25 y 50 columnas, respectivamente.\n","\n","**N-gramas**: si bien Bag of Words y Tf-idf utilizan una palabra por defecto para representar un documento, existe la posibilidad de utilizar conjuntos de palabras. Para el caso de este problema, se decide utilizar bigramas es decir, contar la aparición de conjuntos de 2 palabras.\n","\n","Para construir los experimentos se utilizan distintas combinaciones de estas representaciones. Para emplear algunas de estas características, se crea una clase Transformer para poder usarlas a través de un Pipeline.  \n"]},{"cell_type":"markdown","metadata":{"id":"3y5QVZD6Xqz1","colab_type":"text"},"source":["## 3. Algoritmos"]},{"cell_type":"markdown","metadata":{"id":"uxfKdPGhXqz2","colab_type":"text"},"source":["Los algoritmos de clasificación usados fueron SVC, Regresión Logística, Naive Bayes.\n","\n","Naive Bayes: el clasificador multinomial Naives Bayes se basa en aplicar el teorema de Bayes con fuertes supuestos de independencia. Dado un vector $x=(x_{1},...,x_{n})$ representado n variables independientes en este caso documentos, le asigna la probabilidad\n","\n","$p(C_{k}|x_{1},...,x_{n})$\n","\n","para cada clase $C_{k}$. Para simplificar esta probabilidad, se asume independencia entre las variables, permitiendo el uso del teorema de Bayes. De esta manera se puede descomponer la probabilidad como:\n","\n","$p(C_{k}|x)=\\frac{p(C_{k})p(x|C_{k})}{p(x)}$\n","\n","El denominador es constante, ya que es la probabilidad del documento dado el total de documentos. \n","El numerador, por su parte, es equivalente a la probabilidad conjunta de $p(C_{k},x_{1},...,x_{n})$, y con el supuesto de independencia llegamos a que eso es proporcional a $p(C_{k})\\prod_{i=1}^{n}p(x_{i}|C_{k})$.\n","\n","Dado esto, lo que el clasificador busca es encontrar la clase más probable, para lo cual maximiza la probabilidad antes mencionada con la siguiente función:\n","\n","$y=argmax_{k\\in\\{1,...,K\\}}p(C_{k})\\prod_{i=1}^{n}p(x_{i}|C_{k})$.\n","\n","Regresión Logística: forma parte del conjunto de modelos lineales. Es un algoritmo de clasificación que predice la probabilidad de una variable dependiente de una clase. Dicha variable dependiente es una variable binaria que es 1 cuando es un éxito o un 0 cuando es un fallo. La regresión logística predice la $P(Y=1)$ como una función de $X$. La función logística se puede definir como el logaritmo natural del cuociente entre los eventos de éxito y de fracaso. Para predecir la probabilidad buscada se usa la función sigmoide que se define como la inversa de la función logística:\n","\n","$\\phi(z) = \\frac{1}{1+e^{-z}}$\n","\n","Luego se define una función Loss que representa el error del modelo, y utilizando el método del gradiente descendiente, se puede minimizar iterativamente esta función hasta llegar a un mínimo local.\n","\n","SVC: Support Vector Classifier es un clasificador de la librería sklearn. Este clasificador está basado en el algoritmo de Support Vector Machine. Si bien, este algoritmo no es de naturaleza probabilística, existe una adpatación que le da esa característica, que a su vez la librería implementa. Este algoritmo tiene una interpretación un poco más geométrica, el cual busca un hiperplano que pueda separar de forma óptima los puntos en las distintas categorías del modelo. \n"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-31T13:47:52.064631Z","start_time":"2020-03-31T13:47:52.044451Z"},"id":"F_YUJbfcXqz4","colab_type":"text"},"source":["## 4. Métricas de Evaluación"]},{"cell_type":"markdown","metadata":{"id":"NXoj20HhXqz6","colab_type":"text"},"source":["**AUC:** la sigla significa *Área bajo la curva*, esta métrica calcula el área bajo la curva ROC usando la regla del trapezoide.\n","\n","La curva ROC mencionada (curva de característica operativa del receptor) es un gráfico que muestra el rendimiento de un modelo de clasificación en todos los umbrales de clasificación. Esta curva representa dos parámetros: Recall y Precision (detallados más adelante).\n","\n","Una forma de interpretar el AUC es considerarlo un estimado de la probabilidad que un clasificador rankee más alto una instancia positiva aleatoriamente  que una instancia negativa aleatoriamente.\n","\n","Su valor se encuentra entre 0 y 1. Dando 0 para un modelo sin predicciones correctas y dando 1 para un modelo de predicción perfecto.\n","\n","\n","**Kappa:** es una métrica que compara la exactitud observada del sistema que se quiere medir con la exactitud de un sistema aleatorio que se considera una exactitud esperada.\n","Se define de la siguiente manera: \n","\n","\n","$kappa = \\frac{TotalAccuracy - RandomAccuracy}{1-RandomAccuracy} $\n","\n","\n","Donde TotalAccuracy es la suma de los aciertos sobre el total:\n","\n","\n","$TotalAccuracy = \\frac{TruePos + TrueNeg}{TruePos + TrueNeg + FalsePos +FalseNeg}$\n","\n","\n","Y Random Accuracy se define como la suma de los productos de la probabilidad de referencia y la probabilidad resultante sobre el cuadrado del total:\n","\n","\n","$RandomAccuracy = \\frac{(TrueNeg+FalsePos)*(TrueNeg+FalseNeg)+(TruePos+FalseNeg)*(TruePos+FalsePos)}{Total^2}$\n","\n","\n","\n","Kappa siempre es menor a 1. Aunque no hay una medida estándar para interpretar sus valores, Landis y Koch (1977) indicaron una manera de caracterizar sus valores. De acuerdo a su esquema, un valor menor o igual a cero indica que es un clasificador inútil, entre 0 y 0.20 un resultado ligero, entre 0.21 y 0.40 un resultado decente, entre 0.41-0.60 resultado moderado, 0.61- 0.80 resultado sustancial y 0.81-1 un resultado practicamente perfecto. \n","\n","\n","\n","**Accuracy:** Esta métrica es la suma de los aciertos sobre el total, que indica qué fracción de clasificaciones del sistema son correctas, tal como se mencionó anteriormente y se describe así:\n","\n","$Accuracy = \\frac{TruePos + TrueNeg}{TruePos + TrueNeg + FalsePos +FalseNeg}$\n","\n","Es la clasificación más intuitiva ya que indica el porcentaje de aciertos en el total de casos a clasificar. A partir de aquí se desprenden tres mediciones que entregan distinta información.\n","\n","Recall o sensibilidad es la fracción de instancias que han sido recuperadas y se describe de la siguiente manera:\n","\n","$Recall = \\frac{TruePos }{TruePos +FalseNeg}$\n","\n","Precision es la fracción de instancias que fueron correctamente identificadas, es decir  y se describe así:\n","\n","$Precision = \\frac{TruePos }{TruePos +FalsePos}$\n","\n","Existe un *tradeoff* entre ambas medidas y por eso se puede ponderar ambas en un nuevo valor llamado F1-Score que es la media armónica entre Precision y Recall. Dando igual importancia a ambos la forma del F1-Score es la siguiente:\n","\n","\n","$F_{1}=\\frac{2* Precision* Recall}{Precision+Recall}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MFdDi8b0Xqz6","colab_type":"text"},"source":["## 5. Experimentos"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-31T13:31:40.023344Z","start_time":"2020-03-31T13:31:40.003541Z"},"id":"8l_sM9HPXqz9","colab_type":"text"},"source":["### Importar librerías y utiles"]},{"cell_type":"code","metadata":{"id":"xDMZpoyDPDiv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":402},"outputId":"9416400b-a67f-430e-871c-9e612f960cb6"},"source":["# Para instalar de ser necesario\n","!pip install emoji\n","!pip install regex\n","!pip install vaderSentiment"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n","\r\u001b[K     |███████▌                        | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.0MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=d90127c96ac0d3bc223042f0507d7d4a7f5746ee87deb250c27d0301dde521fa\n","  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.5.4\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n","Collecting vaderSentiment\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n","\u001b[K     |████████████████████████████████| 133kB 3.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vaderSentiment) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (2020.4.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vaderSentiment) (3.0.4)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:20.587160Z","start_time":"2020-04-07T15:44:19.319386Z"},"id":"CyTQF6ItXqz-","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import os\n","import numpy as np\n","import shutil\n","import nltk\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.base import BaseEstimator, TransformerMixin"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d8eutJgzXq0G","colab_type":"text"},"source":["### Definir métodos de evaluación\n","\n","Estas funciones están a cargo de evaluar los resultados de la tarea. \n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:20.604066Z","start_time":"2020-04-07T15:44:20.589106Z"},"id":"QrcfsMTIXq0K","colab_type":"code","colab":{}},"source":["def auc_score(test_set, predicted_set):\n","    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n","    medium_predicted = np.array(\n","        [prediction[1] for prediction in predicted_set])\n","    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n","    high_test = np.where(test_set == 'high', 1.0, 0.0)\n","    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n","    low_test = np.where(test_set == 'low', 1.0, 0.0)\n","    auc_high = roc_auc_score(high_test, high_predicted)\n","    auc_med = roc_auc_score(medium_test, medium_predicted)\n","    auc_low = roc_auc_score(low_test, low_predicted)\n","    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n","             high_test.sum() * auc_high) / (\n","                 low_test.sum() + medium_test.sum() + high_test.sum())\n","    return auc_w\n","\n","\n","def evaulate(predicted_probabilities, y_test, labels, dataset_name):\n","    # Importante: al transformar los arreglos de probabilidad a clases,\n","    # entregar el arreglo de clases aprendido por el clasificador.\n","    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n","    predicted_labels = [\n","        labels[np.argmax(item)] for item in predicted_probabilities\n","    ]\n","    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n","    print(\n","        confusion_matrix(y_test,\n","                         predicted_labels,\n","                         labels=['low', 'medium', 'high']))\n","\n","    print('\\nClassification Report:\\n')\n","    print(\n","        classification_report(y_test,\n","                              predicted_labels,\n","                              labels=['low', 'medium', 'high']))\n","    # Reorder predicted probabilities array.\n","    labels = labels.tolist()\n","    predicted_probabilities = predicted_probabilities[:, [\n","        labels.index('low'),\n","        labels.index('medium'),\n","        labels.index('high')\n","    ]]\n","    auc = round(auc_score(y_test, predicted_probabilities), 3)\n","    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n","    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n","    print(\"Kappa:\", kappa, end='\\t')\n","    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n","    print(\"Accuracy:\", accuracy)\n","    print('------------------------------------------------------\\n')\n","    return np.array([auc, kappa, accuracy])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tz5VTs_gXq0Q","colab_type":"text"},"source":["### Datos\n","\n","Se obtienen los datasets desde el github del curso.\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.068137Z","start_time":"2020-04-07T15:44:20.606061Z"},"id":"12vi557SXq0R","colab_type":"code","colab":{}},"source":["# Datasets de entrenamiento.\n","train = {\n","    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n","    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n","    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n","    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n","}\n","# Datasets que deberán predecir para la competencia.\n","target = {\n","    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n","    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n","    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n","    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U5g_f7ciXq0d","colab_type":"text"},"source":["### Analizar los datos \n","\n","Se imprime la cantidad de tweets de cada dataset, según su intensidad de sentimiento. Notar que las clases están desbalanceadas. "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.117633Z","start_time":"2020-04-07T15:44:21.090703Z"},"id":"YQnCzq11Xq0d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":573},"outputId":"c4d2bc6a-47a3-4d12-e34d-d584eae24759"},"source":["def get_group_dist(group_name, train):\n","    print(group_name, \"\\n\",\n","          train[group_name].groupby('sentiment_intensity').count(),\n","          '\\n---------------------------------------\\n')\n","for dataset_name in train:\n","    get_group_dist(dataset_name, train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["anger \n","                       id  tweet  class\n","sentiment_intensity                   \n","high                 163    163    163\n","low                  161    161    161\n","medium               617    617    617 \n","---------------------------------------\n","\n","fear \n","                       id  tweet  class\n","sentiment_intensity                   \n","high                 270    270    270\n","low                  288    288    288\n","medium               699    699    699 \n","---------------------------------------\n","\n","joy \n","                       id  tweet  class\n","sentiment_intensity                   \n","high                 195    195    195\n","low                  219    219    219\n","medium               488    488    488 \n","---------------------------------------\n","\n","sadness \n","                       id  tweet  class\n","sentiment_intensity                   \n","high                 197    197    197\n","low                  210    210    210\n","medium               453    453    453 \n","---------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KFQH7W89nXvR","colab_type":"text"},"source":["### Balanceo de clases\n","\n","Para obtener mejores resultados se balancean las clases en los distintos conjuntos de entrenamiento. En este caso se utiliza subsampling, dejando como la cantidad de muestras por categoría como el mínimo encontrado en cada conjunto.\n"]},{"cell_type":"code","metadata":{"id":"KKp3Z0xcacDV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"f43616a1-f713-4f53-d105-54233983ca27"},"source":["min_num_samples_anger = 161\n","min_num_samples_fear = 270\n","min_num_samples_joy = 195\n","min_num_samples_sadness = 197\n","\n","an = train['anger'].groupby('sentiment_intensity')\n","train['anger'] = pd.DataFrame(\n","    an.apply(lambda x: x.sample(min_num_samples_anger).reset_index(drop=True))).reset_index(\n","        drop=True)\n","fe = train['fear'].groupby('sentiment_intensity')\n","train['fear'] = pd.DataFrame(\n","    fe.apply(lambda x: x.sample(min_num_samples_fear).reset_index(drop=True))).reset_index(\n","        drop=True)\n","jo = train['joy'].groupby('sentiment_intensity')\n","train['joy'] = pd.DataFrame(\n","    jo.apply(lambda x: x.sample(min_num_samples_joy).reset_index(drop=True))).reset_index(\n","        drop=True)\n","sa = train['sadness'].groupby('sentiment_intensity')\n","train['sadness'] = pd.DataFrame(\n","    sa.apply(lambda x: x.sample(min_num_samples_sadness).reset_index(drop=True))).reset_index(\n","        drop=True)\n","\n","  \n","train['fear'].sentiment_intensity.value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["medium    270\n","low       270\n","high      270\n","Name: sentiment_intensity, dtype: int64"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"nIfyoxUrXq0l","colab_type":"text"},"source":["### Custom Features \n","\n","Para crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n","\n","1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta carácteres relevantes ('!', '?', '#') en los tweets.\n","2. Definios una función cómo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n","3. Hacemos un override de la función `transform` en donde iteramos por cada tweet, llamamos a la función que hicimos antes y agregamos sus resultados a un arrelo. Finalmente lo retornamos.\n","\n","Esto nos facilitará el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aquí](https://scikit-learn.org/stable/data_transforms.html).\n","\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.128600Z","start_time":"2020-04-07T15:44:21.119624Z"},"id":"Ev8s-pFjXq0l","colab_type":"code","colab":{}},"source":["\n","import emoji\n","import regex\n","class CharsCountTransformer(BaseEstimator, TransformerMixin):\n","\n","    def get_relevant_chars(self, tweet):\n","        num_hashtags = tweet.count('#')\n","        num_exclamations = tweet.count('!')\n","        num_interrogations = tweet.count('?')\n","        return [num_hashtags, num_exclamations, num_interrogations]\n","\n","    def transform(self, X, y=None):\n","        chars = []\n","        for tweet in X:\n","            chars.append(self.get_relevant_chars(tweet))\n","\n","        return np.array(chars)\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","class EmojiCountTransformer(BaseEstimator, TransformerMixin):\n","    def count_happy_emojis(self, tweet):\n","        count = 0\n","        happy_emojis = \"😀😃😄😁😆😅🤣😂🙂😊😉🤩☺😋😌🥳😎😺😸😹🙉\"\n","        for c in tweet:\n","            if c in happy_emojis:\n","                count += 1\n","\n","        return count\n","\n","    def count_sad_emojis(self, tweet):\n","        count = 0\n","        sad_emojis = \"😔😪😷🤕🤢😟🙁☹🥺😢😭😞😿\"\n","        for c in tweet:\n","            if c in sad_emojis:\n","                count += 1\n","\n","        return count\n","\n","    def count_fear_emojis(self, tweet):\n","        count = 0\n","        fear_emojis = \"😐😶😟😦😧😰😥😱😩😫🙀\"\n","        for c in tweet:\n","            if c in fear_emojis:\n","                count += 1\n","\n","        return count\n","\n","    def count_anger_emojis(self, tweet):\n","        count = 0\n","        anger_emojis = \"🤔😑😒😡😠🤬👿😾\"\n","        for c in tweet:\n","            if c in anger_emojis:\n","                count += 1\n","\n","        return count\n","    \n","    def get_relevant_emojis(self, tweet):\n","        num_happy = self.count_happy_emojis(tweet)\n","        num_sad = self.count_sad_emojis(tweet)\n","        num_fear = self.count_fear_emojis(tweet)\n","        num_anger = self.count_anger_emojis(tweet)\n","        return [num_happy, num_sad, num_anger, num_anger]\n","    \n","    def transform(self, X, y=None):\n","        emojis = []\n","        for tweet in X:\n","            emojis.append(self.get_relevant_emojis(tweet))\n","\n","        return np.array(emojis)\n","\n","    def fit(self, X, y=None):\n","        return self\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3px7KvhI77CH","colab_type":"code","colab":{}},"source":["import string\n","class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n","    \"\"\" Transforma tweets a representaciones vectoriales usando algún modelo de Word Embeddings.\n","    \"\"\"\n","    \n","    def __init__(self, model, aggregation_func):\n","        self.model = model\n","        \n","        self.aggregation_func = aggregation_func\n","\n","    def simple_tokenizer(self, doc, lower=False):\n","        \"\"\"Tokenizador. Elimina signos de puntuación, lleva las letras a minúscula(opcional) y \n","           separa el tweet por espacios.\n","        \"\"\"\n","        if lower:\n","            doc.translate(str.maketrans('', '', string.punctuation)).lower().split()\n","        return doc.translate(str.maketrans('', '', string.punctuation)).split()\n","\n","    def transform(self, X, y=None):\n","        \n","        doc_embeddings = []\n","        \n","        for doc in X:\n","            tokens = self.simple_tokenizer(doc, lower = True) \n","            \n","            selected_wv = []\n","            for token in tokens:\n","                if token in self.model.vocab:\n","                    selected_wv.append(self.model[token])\n","                    \n","            # si seleccionamos por lo menos un embedding para el tweet, lo agregamos y luego lo añadimos.\n","            if len(selected_wv) > 0:\n","                doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n","                doc_embeddings.append(doc_embedding)\n","            # si no, añadimos un vector de ceros que represente a ese documento.\n","            else:\n","                doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo \n","\n","        return np.array(doc_embeddings)\n","\n","    def fit(self, X, y=None):\n","        return self"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYHEln83jDjk","colab_type":"code","colab":{}},"source":["class VaderPolarityTransformer(BaseEstimator, TransformerMixin):\n","    def __init__(self, analyzer):\n","        self.analyzer = analyzer\n","\n","    def get_polarities(self, doc):\n","        score = self.analyzer.polarity_scores(doc)\n","        return [score['pos'], score['neu'], score['neg'], score['compound']]\n","\n","    def transform(self, X, y=None):\n","        polarities = []\n","        for tweet in X:\n","            polarities.append(self.get_polarities(tweet))\n","\n","        return np.array(polarities)\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f2RaQKQotpc5","colab_type":"code","colab":{}},"source":["import re\n","class ElongatedWordsAndPunctuationsTransformer(BaseEstimator, TransformerMixin):\n","    def get_elongated_words_count(self, tokens):\n","        count = 0\n","        elong = re.compile(\"([a-zA-Z])\\\\1{2,}\")\n","        for token in tokens:\n","            if elong.search(token) != None:\n","                count += 1\n","        return count\n","\n","    def get_punctuation_marks(self, tokens):\n","        count = 0\n","        elong = re.compile(\"([,.¡!¿?])\\\\1{1,}\")\n","        for token in tokens:\n","            if elong.search(token)!=None:\n","                count += 1\n","        return count\n","\n","    def get_elongated_count(self, doc):\n","        elongated_words = self.get_elongated_words_count(doc.split())\n","        punctuation_marks = self.get_punctuation_marks(doc.split())\n","        return [elongated_words, punctuation_marks]\n","\n","    def transform(self, X, y=None):\n","        marks = []\n","        for tweet in X:\n","            marks.append(self.get_elongated_count(tweet))\n","\n","        return np.array(marks)\n","\n","    def fit(self, X, y=None):\n","        return self"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"83Oq1mTBXq0u","colab_type":"text"},"source":["### Definir la representación y el clasificador\n","\n","Para esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando así nuestra programación.\n","\n","El pipeline más básico que podemos hacer es transformar el dataset a Bag of Words y después usar clasificar el BoW usando NaiveBayes:\n","\n","```python\n","    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n","```\n","\n","\n","Ahora, si queremos usar nuestra transformación para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenará los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n","\n","```python\n","    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n","                                        ('chars_count',CharsCountTransformer())])),\n","              ('clf', MultinomialNB())])\n","\n","```\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FrsgAq1pXq0w","colab_type":"text"},"source":["Recuerden que cada pipeline representa un sistema de clasificación distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podrían solapar resultados.  Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:"]},{"cell_type":"code","metadata":{"id":"2Uq_CiCT-YYj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"dfeb5b9d-10f1-4ecd-81b7-63cc16c624f4"},"source":["import gensim.downloader as api\n","# Se sugiera correr esta celda una vez pues toma tiempo descargar los modelos\n","model = api.load(\"glove-twitter-25\")  \n","model2 = api.load(\"glove-twitter-50\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[=================================-----------------] 66.4% 69.6/104.8MB downloaded\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["[================================================--] 96.4% 192.4/199.5MB downloaded\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zvET_n3k9CQv","colab_type":"text"},"source":["### Resumen de experimentos\n","\n","Los experimentos consisten en hacer combinaciones de las representaciones mencionadas en esa sección del informe más el uso de alguno de los algoritmos de clasificación. A continuación se detallan aspectos generales que marcan la diferencia a la hora de comparar los resultados de los experimentos:\n","\n","**Algoritmo escogido**: si bien se utilizaron 3 algoritmos de clasificación, el clasificador basado en Regresión Logística fue el que obtuvo mejores resultados. Si bien Naive Bayes muestra resultados aceptables, al tener su supuesto de independencia y basado en vectores de frecuencia, no es compatible con otras características como Word Embedding o los puntajes otorgados por Vader. Por otro lado, SVC no es un algoritmo de origen probabilístico, por lo que tal vez para este problema, aun cuando está adaptado para arrojar probabilidades, no lo hace de manera eficaz.\n","\n","**Librería Vader**: se evidencia un mejora de resultados al utilizar la librería Vader. En particular, se registran los mejores resultados en la felicidad y tristeza. Al no utilizar esta librería se visualiza una pequeña baja en las métricas.\n","\n","**Word Embedding**: por sí solo, utilizar Word Embedding es mejor que utilizar alguna de las otras 2 representaciones vectoriales (Bag of Words o Tf-idf). No se muestran diferencias sustanciales entre usar el modelo de 25 o 50 dimensiones, por lo que se prefiere el de 25. El Transformer que se utiliza (extraído de la Clase Auxiliar 2) requiere de una función de agregación (que permitiera unir o representar en 1 vector los vectores de todos las palabras de un tweet). En este caso, la función que mejores resultados obtiene es la del promedio de cada dimensión. Finalmente, no se usa esta representación por sí sola. Mediante los experimentos se evidencia que en conjunto con el vectorizador Tf-idf se obtienen mejores resultados.\n","\n","**Tf-idf**: el uso del vectorizador de Tf-idf muestra mejores resultados que el uso de Bag of Words, pero no mejores que con el uso del Word Embedding.\n","\n","Uso de emoticones, palabras alargadas no mostraron mayores diferencias en los resultados. Aun así, se incluyeron en varios de los experimentos.\n","\n","Para poder comparar resultados cada experimento se realiza diez veces y sus datos son recogidos con tal de generar una mediana y un promedio por cada emoción y para cada métrica. A continuación se detallan los clasificadores y la combinación de características junto con el promedio general de resultados obtenidos por cada experimento.\n","\n","\n","### Experimentos realizados\n","| Resumen de experimentos | Características                                                                                                                                                                                                                                                                                                           | Clasificador                                  | AUC Promedio | Kappa Promedio | Accuracy Promedio |\n","|-------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|--------------|----------------|-------------------|\n","| 1                       | Vector de Tf-idf.                                                                                                                                                                                                                                                                                                         | Naive Bayes                                   |       0.7293 |         0.3044 |            0.5352 |\n","| 2                       | Vector de Bag of Words, frecuencia de caracteres especiales, <br>frecuencia de emoticones.                                                                                                                                                                                                                                | Regresión Logística                           |       0.6445 |         0.1895 |            0.4509 |\n","| 3                       | Vector del promedio de los vectores de las palabras según el<br>Word Embedding de 25 dimensiones, puntajes de Vader, frecuencia<br>de caracteres especiales, frecuencia de emoticones, frecuencia<br>de palabras alargadas.                                                                                               | Regresión Logística                           |       0.7551 |         0.3339 |            0.5581 |\n","| 4                       | Igual que experimento 3 más vector de Bag of Words.                                                                                                                                                                                                                                                                       | Regresión Logística                           |       0.7757 |         0.3869 |            0.5906 |\n","| 5                       | Puntajes de Vader.                                                                                                                                                                                                                                                                                                        | Regresión Logística                           |       0.7158 |         0.3216 |            0.5452 |\n","| 6                       | Igual que experimento 4 salvo que se usa vector de Tf-idf en vez<br>de Bag of Words.                                                                                                                                                                                                                                      | Regresión Logística                                           |       0.7771 |         0.3957 |            0.5954 |\n","| 7                       | Vector de Bag of Words, vector resultante del promedio de cada<br>dimensión de vectores de las palabras según el Word Embedding<br>de 25 dimensiones, puntajes de Vader, frecuencia de caracteres <br>especiales, frecuencia de emoticones, frecuencia de palabras <br>alargadas.                                         | SVC                           |       0.7534 |   0.3325555556 |            0.5558 |\n","| 8                       | Igual que 6 salvo que se usa función de máximo en agregación <br>de vectores de Word Embedding.                                                                                                                                                                                                                           | Regresión Logística                           |       0.7631 |   0.3593333333 |            0.5731 |\n","| 9                       | Igual que 6 salvo que se usa función de suma en agregación <br>de vectores de Word Embedding                                                                                                                                                                                                                              | Regresión Logística                           |        0.773 |          0.385 |            0.5898 |\n","| 10                      | Vector de Tf-idf considerando bigramas, vector de Bag of Words, <br>vector resultante de la suma de cada dimensión de vectores de <br>las palabras según el Word Embedding de 25 dimensiones, puntajes<br>de Vader, frecuencia de caracteres especiales, frecuencia de <br>emoticones, frecuencia de palabras alargadas.  | Regresión Logística                           |        0.768 |   0.3673333333 |            0.5791 |\n","| 11                      | Vector de Tf-idf considerando bigramas, vector de Bag of Words, <br>vector resultante de la suma de cada dimensión de vectores de <br>las palabras según el Word Embedding de 25 dimensiones, puntajes <br>de Vader, frecuencia de caracteres especiales, frecuencia de <br>emoticones, frecuencia de palabras alargadas. | Regresión Logística                           |       0.7762 |   0.3866666667 |             0.589 |\n","| 12                      | Vector de Tf-idf, vector resultante de la suma de cada dimensión <br>de vectores de las palabras según el Word Embedding de 25 <br>dimensiones, puntajes de Vader.                                                                                                                                                        | Regresión Logística                           |       0.7776 |   0.3866666667 |            0.5898 |\n","| 13                      | Vector de Tf-idf.                                                                                                                                                                                                                                                                                                         | Regresión Logística                           |       0.7244 |   0.3074444444 |             0.531 |\n","| 14                      | Vector de Tf-idf y vector de Word Embedding con función del <br>promedio.                                                                                                                                                                                                                                                 | Regresión Logística                           |       0.7255 |   0.2953333333 |            0.5286 |\n","| 15                      | Igual que experimento 6.                                                                                                                                                                                                                                                                                                  | Regresión Logística <br>(2000000 iteraciones) |       0.7731 |           0.38 |            0.5827 |\n","| 16                      | Igual que experimento 6.                                                                                                                                                                                                                                                                                                  | Regresión Logística <br>(1000 iteraciones)    |       0.7768 |   0.3894444444 |            0.5941 |\n","| 17                      | Igual que experimento 6 salvo que se usó modelo de Word <br>Embedding de vectores de 50 dimensiones.                                                                                                                                                                                                                      | Regresión Logística                           |       0.7753 |   0.3823333333 |            0.5855 |"]},{"cell_type":"markdown","metadata":{"id":"X-2YLDYHZV_N","colab_type":"text"},"source":["En seguida se presenta un resumen de los mejores resultados por emoción para cada métrica donde EX representa el número de experimento que obtuvo el el resultado"]},{"cell_type":"markdown","metadata":{"id":"bo3DJhosXIVI","colab_type":"text"},"source":["| Mejores Resultados |              |    |                |    |                   |       |\n","|--------------------|--------------|----|----------------|----|-------------------|-------|\n","|                    |              |    |                |    |                   |       |\n","| Resumen Promedios  | AUC Promedio | EX | Kappa Promedio | EX | Accuracy Promedio | EX    |\n","| Promedios Anger    |       0.7681 |  4 |          0.379 |  4 |            0.5863 |     4 |\n","| Promedios Fear     |       0.7835 | 12 |         0.3992 | 12 |            0.5992 |    12 |\n","| Promedios Joy      |       0.8183 | 10 |         0.4638 | 10 |            0.6422 |    10 |\n","| Promedios Sadness  |       0.7721 |  6 |         0.3807 |  6 |            0.5872 |     6 |\n","| Promedios Promedio |       0.7776 | 12 |         0.3957 |  6 |            0.5954 |     6 |\n","|                    |              |    |                |    |                   |       |\n","|                    |              |    |                |    |                   |       |\n","| Resumen Medias     | AUC Promedio | EX | Kappa Promedio | EX | Accuracy Promedio | EX    |\n","| Media Anger        |        0.772 |  4 |         0.3795 | 16 |            0.5875 |    16 |\n","| Media Fear         |       0.7805 | 12 |         0.4095 | 12 |             0.606 |    12 |\n","| Media Joy          |        0.823 | 10 |         0.4755 |  9 |             0.644 | 10-17 |\n","| Media Sadness      |       0.7805 |  6 |         0.3805 | 17 |             0.587 |    17 |\n","| Media Promedio     |        0.778 | 17 |         0.3985 |  6 |            0.5985 |    16 |"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.155528Z","start_time":"2020-04-07T15:44:21.149545Z"},"id":"cuulTk71Xq0x","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","analyzer = SentimentIntensityAnalyzer()\n","\n","def get_experiment_0_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('bow', CountVectorizer()),\n","                                    ('chars_count', CharsCountTransformer())\n","                                    ])), ('clf', MultinomialNB())])\n","\n","def get_experiment_1_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer())\n","                                    ])), ('clf', MultinomialNB())])\n","\n","def get_experiment_2_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('bow', CountVectorizer(ngram_range=(3,3))),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer())\n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","\n","\n","\n","def get_experiment_3_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","\n","\n","def get_experiment_4_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('bow', CountVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","    \n","def get_experiment_5_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('vader', VaderPolarityTransformer(analyzer))\n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","\n","\n","# el mejor hasta ahora\n","def get_experiment_6_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","\n","def get_experiment_7_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', SVC(probability=True))])\n","    \n","def get_experiment_8_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.max)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_9_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.sum)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_10_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer(ngram_range=(2,2))),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_11_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer(ngram_range=(1,2))),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_12_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer))\n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_13_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer())\n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_14_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean))\n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n","    \n","def get_experiment_15_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=2000000))])\n","    \n","def get_experiment_16_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000))])\n","\n","def get_experiment_17_pipeline():\n","    return Pipeline([('features',\n","                      FeatureUnion([('tfidf', TfidfVectorizer()),\n","                                    ('embedding', Doc2VecTransformer(model2, np.mean)),\n","                                    ('vader', VaderPolarityTransformer(analyzer)),\n","                                    ('chars_count', CharsCountTransformer()),\n","                                    ('emojis_count', EmojiCountTransformer()),\n","                                    ('elongated', ElongatedWordsAndPunctuationsTransformer()),\n","                                    \n","                                    ])), ('clf', LogisticRegression(max_iter=1000000))])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4eL9AYayXq05","colab_type":"text"},"source":["### Ejecutar el pipeline para algún dataset"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.167498Z","start_time":"2020-04-07T15:44:21.157540Z"},"scrolled":true,"id":"fS1vWn2mXq05","colab_type":"code","colab":{}},"source":["def run(dataset, dataset_name, pipeline):\n","    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n","    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n","\n","    # Dividimos el dataset en train y test.\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        dataset.tweet,\n","        dataset.sentiment_intensity,\n","        shuffle=True,\n","        test_size=0.33)\n","\n","\n","    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline)\n","    pipeline.fit(X_train, y_train)\n","\n","    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n","    predicted_probabilities = pipeline.predict_proba(X_test)\n","\n","    # Obtenemos el orden de las clases aprendidas.\n","    learned_labels = pipeline.classes_\n","\n","    # Evaluamos:\n","    scores = evaulate(predicted_probabilities, y_test, learned_labels, dataset_name)\n","    return pipeline, learned_labels, scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCh7ffVtXq08","colab_type":"text"},"source":["### Ejecutar el sistema creado por cada train set\n","\n","Este código crea y entrena los 4 sistemas de clasificación y luego los evalua. Para los experimentos, pueden copiar este código variando el pipeline cuantas veces estimen conveniente."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.384119Z","start_time":"2020-04-07T15:44:21.170488Z"},"scrolled":false,"id":"vxtKeEZ2Xq0-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7d3f7efa-cfd2-432c-c5be-bbeb24221679"},"source":["classifiers = []\n","learned_labels_array = []\n","scores_array = []\n","\n","# Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n","for dataset_name, dataset in train.items():\n","    \n","    # creamos el pipeline\n","    # cambiar según experimento a ejecutar\n","    pipeline = get_experiment_15_pipeline()\n","    \n","    # ejecutamos el pipeline sobre el dataset\n","    classifier, learned_labels, scores = run(dataset, dataset_name, pipeline)\n","\n","    # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n","    classifiers.append(classifier)\n","\n","    # guardamos las labels aprendidas por el clasificador\n","    learned_labels_array.append(learned_labels)\n","\n","    # guardamos los scores obtenidos\n","    scores_array.append(scores)\n","\n","# print avg scores\n","print(\n","    \"Average scores:\\n\\n\",\n","    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n","    .format(*np.array(scores_array).mean(axis=0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Confusion Matrix for anger:\n","\n","[[35 15  7]\n"," [12 20 24]\n"," [ 3  8 36]]\n","\n","Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","         low       0.70      0.61      0.65        57\n","      medium       0.47      0.36      0.40        56\n","        high       0.54      0.77      0.63        47\n","\n","    accuracy                           0.57       160\n","   macro avg       0.57      0.58      0.56       160\n","weighted avg       0.57      0.57      0.56       160\n","\n","Scores:\n","\n","AUC:  0.778\tKappa: 0.358\tAccuracy: 0.569\n","------------------------------------------------------\n","\n","Confusion Matrix for fear:\n","\n","[[59 22  7]\n"," [29 38 25]\n"," [14 20 54]]\n","\n","Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","         low       0.58      0.67      0.62        88\n","      medium       0.47      0.41      0.44        92\n","        high       0.63      0.61      0.62        88\n","\n","    accuracy                           0.56       268\n","   macro avg       0.56      0.57      0.56       268\n","weighted avg       0.56      0.56      0.56       268\n","\n","Scores:\n","\n","AUC:  0.75\tKappa: 0.346\tAccuracy: 0.563\n","------------------------------------------------------\n","\n","Confusion Matrix for joy:\n","\n","[[48 13  3]\n"," [11 39 12]\n"," [ 5 15 48]]\n","\n","Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","         low       0.75      0.75      0.75        64\n","      medium       0.58      0.63      0.60        62\n","        high       0.76      0.71      0.73        68\n","\n","    accuracy                           0.70       194\n","   macro avg       0.70      0.69      0.70       194\n","weighted avg       0.70      0.70      0.70       194\n","\n","Scores:\n","\n","AUC:  0.856\tKappa: 0.544\tAccuracy: 0.696\n","------------------------------------------------------\n","\n","Confusion Matrix for sadness:\n","\n","[[42 17  6]\n"," [25 23 24]\n"," [ 6 11 42]]\n","\n","Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","         low       0.58      0.65      0.61        65\n","      medium       0.45      0.32      0.37        72\n","        high       0.58      0.71      0.64        59\n","\n","    accuracy                           0.55       196\n","   macro avg       0.54      0.56      0.54       196\n","weighted avg       0.53      0.55      0.53       196\n","\n","Scores:\n","\n","AUC:  0.737\tKappa: 0.323\tAccuracy: 0.546\n","------------------------------------------------------\n","\n","Average scores:\n","\n"," Average AUC: 0.78\t Average Kappa: 0.393\t Average Accuracy: 0.593\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2019-08-21T19:37:43.169737Z","start_time":"2019-08-21T19:37:43.166744Z"},"id":"kggQZAa6Xq1I","colab_type":"text"},"source":["### Predecir los target set y crear la submission\n","\n","Aquí predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.392097Z","start_time":"2020-04-07T15:44:21.386114Z"},"id":"mGmF-dOkXq1J","colab_type":"code","colab":{}},"source":["def predict_target(dataset, classifier, labels):\n","    # Predecir las probabilidades de intensidad de cada elemento del target set.\n","    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n","    # Agregar ids\n","    predicted['id'] = dataset.id.values\n","    # Reordenar las columnas\n","    predicted = predicted[['id', 'low', 'medium', 'high']]\n","    return predicted"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-07T15:44:21.588573Z","start_time":"2020-04-07T15:44:21.394094Z"},"scrolled":true,"id":"bvhd4VsPXq1N","colab_type":"code","colab":{}},"source":["predicted_target = {}\n","\n","# Crear carpeta ./predictions\n","if (not os.path.exists('./predictions')):\n","    os.mkdir('./predictions')\n","\n","else:\n","    # Eliminar predicciones anteriores:\n","    shutil.rmtree('./predictions')\n","    os.mkdir('./predictions')\n","\n","# por cada target set:\n","for idx, key in enumerate(target):\n","    # Predecirlo\n","    predicted_target[key] = predict_target(target[key], classifiers[idx],\n","                                           learned_labels_array[idx])\n","    # Guardar predicciones en archivos separados. \n","    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n","                                 sep='\\t',\n","                                 header=False,\n","                                 index=False)\n","\n","# Crear archivo zip\n","a = shutil.make_archive('predictions', 'zip', './predictions')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-UHyPIXcXq1R","colab_type":"text"},"source":["## 6. Conclusiones"]},{"cell_type":"markdown","metadata":{"id":"UMOuIeXJXq1S","colab_type":"text"},"source":["Como se puede ver en la sección de resultados, los mejores experimentos en el resultado general de las tres métricas, según promedios, son el número 12 en AUC y el 6 en kappa y accuracy. En cuanto al resultado general de la mediana los mejores son el 17 en AUC, el 6 en kappa y el 16 en accuracy. \n"," \n","Lo anterior quiere decir que si se realizan muchos experimentos se puede esperar mejores resultados en promedio para los experimentos 12 y 6. Pero que si se hacen menos experimentos se puede esperar un mejor resultado para los experimentos 17, 6 y 16, ya que tener una mejor mediana significa que son más estables en esa métrica.\n"," \n"," \n","Pero si se observan los experimentos más exitosos por sentimiento, en cuanto a promedio, en las tres métricas fueron el número 4 para anger, el número 12 para fear, el número 10 para Joy y el número 6 para sadness. En cuanto a mediana de los resultados en las tres métricas, los mejores experimentos son el 4 (AUC) y 16 (kappa y accuracy) para anger, 12 para fear, 10 (AUC y accuracy) 9 (kappa) y 17 (accuracy) para joy, 6 (AUC) y 17 (kappa y accuracy) para sadness. \n","Esta diversidad de experimentos indica que hay combinaciones de características que funcionan mejor para un sentimiento determinado. Pero se puede notar que todos los experimentos mencionados comparten el uso de la Regresión Logística como clasificador. Además, que los mejores experimentos (6, 12, 16 y 17) comparten el uso de Tf-idf, Word Embedding y Vader. En particular, el 12 sólo ocupa esas tres cosas obteniendo resultados similares a los demás que tiene más características agregadas. El 6 es el experimento base para el 16 y 17 que usan la misma combinación de características pero con modificaciones en la cantidad de iteraciones en la regresión lineal o en la cantidad de dimensiones de los vectores de Word Embedding.\n"," \n"," \n","Los diversos experimentos demostraron que más características no implica necesariamente mejorar los resultados. Agregar características que no proporcionen información relevante puede perjudicar el modelo. Sin embargo, determinar características relevantes, es decir que logren entregar información sobre las diferencias entre clases, mejora los resultados.\n"," \n"," \n","Dentro del uso de clasificadores, de los tres que se utilizan, el que entrega mejores resultados es el de Regresión Lineal, superando a Naive Bayes y SVC.\n"," \n"," \n","Con respecto a el balance entre las clases, el undersampling resulta considerablemente mejor en la práctica de lo que se espera en teoría. En teoría para clases con menos de mil datos el undersampling puede quitar mucha información valiosa dado que no hay tanto material. Si bien, normalmente se recomienda usar undersampling para clases con decenas de miles de datos, al usarlo se obtuvo mejores resultados en las métricas. De lo que se concluye que se obtiene mejor resultados en las métricas utilizadas con clases balanceadas aunque se use undersampling para clases con menos de mil datos.\n"," \n"," \n","Para un nuevo experimento se propone hacer uso de oversampling sobre los datos de entrenamiento luego de haberlos separado de los datos para testear.\n"," \n"," \n","También se propone usar diferentes combinaciones para cada sentimiento, luego de los resultados obtenidos podemos notar que el usar un clasificador y una combinación particular puede entregar excelentes resultados para un sentimiento pero malos resultados para otro. Al detectar los mejores experimentos para cada sentimiento se puede maximizar los resultados obtenidos por cada sentimiento si se crea un pipeline con diferentes características y clasificador dependiendo del sentimento.\n"," \n"," \n","Luego de comparar los resultados en la competencia se evidencian buenos resultados en las métricas de AUC y kappa, pero pobres resultados en Accuracy. Durante los experimentos el equipo se enfoca más al uso de herramientas probabilísticas lo que provoca una falta en el uso de diccionarios de palabras relacionadas a los sentimientos, esto se ve reflejado en la baja de puntuación en Accuracy. Se propone para mejorar el experimento el uso de sentiment lexicon que puede proporcionar polaridades para distintos conjuntos de palabras relacionadas a cada sentimiento.\n"," \n"," \n","Además, se nota que los mejores resultados se obtuvieron en el sentimiento Joy, pero pobres resultados en Anger, lo que atribuimos a la clasificación que hace Vader sobre si una oración es positiva, neutra o negativa. Para un futuro trabajo recomendamos buscar nuevas herramientas para representar mejor los sentimientos con menores resultados, en este caso Anger.\n"," \n"," \n","Finalmente, concluimos que se cumplieron los objetivos, se mejoró los resultados en kappa y AUC del baseline entregado por el equipo docente. Si bien no se mejoró lo suficiente el resultado en accuracy, se realizaron gran diversidad de experimentos para probar diferentes combinaciones de características y clasificadores, como se esperaba por parte del equipo docente. Y se propusieron mejoras para trabajos futuros a partir de los resultados obtenidos.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U_pXY-mJohPo","colab_type":"text"},"source":["## Anexos\n","\n","1. Tabla completa de resultados obtenidos por cada experimento con tablas de resumen general y para cada experimento: \n","\n","https://docs.google.com/spreadsheets/d/1ImCuFPfhzCb9LSVKL4aLMPYJDnhzTagkIrmmUKsAZC4/edit?usp=sharing"]},{"cell_type":"code","metadata":{"id":"VodiooLQoWV0","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}